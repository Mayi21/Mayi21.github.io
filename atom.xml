<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>wiki</title>
  
  <subtitle>my hearts will go on!</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://mayi21.github.io/"/>
  <updated>2019-05-22T11:24:07.094Z</updated>
  <id>https://mayi21.github.io/</id>
  
  <author>
    <name>Mayi21</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>java笔记-1</title>
    <link href="https://mayi21.github.io/2019/05/22/Java-1/"/>
    <id>https://mayi21.github.io/2019/05/22/Java-1/</id>
    <published>2019-05-22T10:47:49.286Z</published>
    <updated>2019-05-22T11:24:07.094Z</updated>
    
    <content type="html"><![CDATA[<p>Q.String,StringBuffer和StringBuilder的区别<br>A.String是字符串常量，而StringBuilder和StringBuffer是字符串变量；由String创建的字符内容不可改变，而由StringBuffer和StringBuilder创建的字符内容可以改变；StringBuffer是线程安全的，StringBuilder是线程不安全的，但在可能的情况下先建议使用此，因为不需要考虑同步问题，<a id="more"></a>所以会更快，String是线程安全的，但是线程安全主要是针对那些需要维护内部状态改变的类，而String是不可以改变的，所有操作都不会对它进行修改，所以一般不讨论String的线程安全问题。<br>Q.HashSet怎样保证所存元素单一<br>A.HashSet的put方法源码为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">public boolean add(E e) &#123;</span><br><span class="line">return map.put(e, PRESENT)==null;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>其中map为HashMap，PRESENT是final Object。<br>HashMap的put源码为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">public V put(K key, V value) &#123;</span><br><span class="line">return putVal(hash(key), key, value, false, true);</span><br><span class="line">&#125;</span><br><span class="line">/**</span><br><span class="line">    * Implements Map.put and related methods</span><br><span class="line">    *</span><br><span class="line">    * @param hash key的hash值</span><br><span class="line">    * @param key 就是key</span><br><span class="line">    * @param value 就是value</span><br><span class="line">    * @param onlyIfAbsent if true, don&apos;t change existing value(如果为true，就不能改变已经存在的值)</span><br><span class="line">    * @param evict if false, the table is in creation mode(如果为false，表就在创造模式下)</span><br><span class="line">    * @return previous value, or null if none(返回先前的值或者null，如果输入的值为none)</span><br><span class="line">    */</span><br><span class="line">   final V putVal(int hash, K key, V value, boolean onlyIfAbsent,</span><br><span class="line">                  boolean evict) &#123;</span><br><span class="line">       Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i;</span><br><span class="line">       if ((tab = table) == null || (n = tab.length) == 0)</span><br><span class="line">           n = (tab = resize()).length;</span><br><span class="line">       if ((p = tab[i = (n - 1) &amp; hash]) == null)</span><br><span class="line">           tab[i] = newNode(hash, key, value, null);</span><br><span class="line">       else &#123;</span><br><span class="line">           Node&lt;K,V&gt; e; K k;</span><br><span class="line">           if (p.hash == hash &amp;&amp;</span><br><span class="line">               ((k = p.key) == key || (key != null &amp;&amp; key.equals(k))))</span><br><span class="line">               e = p;</span><br><span class="line">           else if (p instanceof TreeNode)</span><br><span class="line">               e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value);</span><br><span class="line">           else &#123;</span><br><span class="line">               for (int binCount = 0; ; ++binCount) &#123;</span><br><span class="line">                   if ((e = p.next) == null) &#123;</span><br><span class="line">                       p.next = newNode(hash, key, value, null);</span><br><span class="line">                       if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st</span><br><span class="line">                           treeifyBin(tab, hash);</span><br><span class="line">                       break;</span><br><span class="line">                   &#125;</span><br><span class="line">                   if (e.hash == hash &amp;&amp;</span><br><span class="line">                       ((k = e.key) == key || (key != null &amp;&amp; key.equals(k))))</span><br><span class="line">                       break;</span><br><span class="line">                   p = e;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">           if (e != null) &#123; // existing mapping for key</span><br><span class="line">               V oldValue = e.value;</span><br><span class="line">               if (!onlyIfAbsent || oldValue == null)</span><br><span class="line">                   e.value = value;</span><br><span class="line">               afterNodeAccess(e);</span><br><span class="line">               return oldValue;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       ++modCount;</span><br><span class="line">       if (++size &gt; threshold)</span><br><span class="line">           resize();</span><br><span class="line">       afterNodeInsertion(evict);</span><br><span class="line">       return null;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Q.String,StringBuffer和StringBuilder的区别&lt;br&gt;A.String是字符串常量，而StringBuilder和StringBuffer是字符串变量；由String创建的字符内容不可改变，而由StringBuffer和StringBuilder创建的字符内容可以改变；StringBuffer是线程安全的，StringBuilder是线程不安全的，但在可能的情况下先建议使用此，因为不需要考虑同步问题，
    
    </summary>
    
      <category term="Java" scheme="https://mayi21.github.io/categories/Java/"/>
    
    
      <category term="Java开发" scheme="https://mayi21.github.io/tags/Java%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>Centos配置hadoop和spark</title>
    <link href="https://mayi21.github.io/2019/03/23/Linux%E9%85%8D%E7%BD%AEhadoop%E4%B8%8Espark/"/>
    <id>https://mayi21.github.io/2019/03/23/Linux配置hadoop与spark/</id>
    <published>2019-03-23T13:05:17.152Z</published>
    <updated>2019-03-24T03:59:35.673Z</updated>
    
    <content type="html"><![CDATA[<p>前言:大数据学生一枚,hadoop是基础,spark是进阶,到了学spark的时候了,又得重新配置一台服务器了.正好借这次机会,把配置步骤记下来,省得以后再翻老班发的PPT.<br>环境:Centos7 64位,JDK1.8 hadoop2.8.4,scala2.11.6,spark2.4.0<a id="more"></a><br>1.先安装通用工具:yum install lrzsz.安装完成后在Xshell中使用rz命令可选择上传文件,十分方便.<br>2.配置网关:<code>vim /etc/sysconfig/network</code>,应该是空的,添加内容如下<code>HOSTNAME=zzti</code> <code>GATEWAY=192.168.17.2</code>,网关设置也可能不是,自己可能要改一下.<br>3.修改主机名称:<code>vim /etc/hosts</code>,在最前面添加<code>127.0.0.1 zzti</code>.  然后关闭防火墙<code>systemctl stop firewalld</code> 查看防火墙状态的命令:<code>systemctl status firewalld</code>.<br>4.此时重启一下,倒杯水,准备下面一番苦干吧.<code>reboot</code>.<br>5.创建工作目录:<code>mkdir -p /data/zzti</code> <code>ln -s /data/zzti/ /zzti</code> <code>useradd zzti</code> <code>chown zzti:zzti /zzti</code>.<br>6.配置免密登录:<code>ssh-keygen</code>,一路回车到底,然后执行<code>cat ~/.ssh/id_rsa.pub &gt; ~/.ssh/authorized_keys</code>  <code>chmod 644 ~/.ssh/authorized_keys</code>  <code>mkdir /home/zzti/.ssh</code> <code>cp ~/.ssh/*  /home/zzti/.ssh/</code> <code>chown -R zzti:zzti /home/zzti/.ssh</code> <code>chmod 700 /home/zzti/.ssh</code>,此过程应该没什么错误.<br><strong>下面操作在zzti用户下完成 <code>su zzti</code></strong><br>7.上传JDK（此处省略无数字,自己找jdk,上传命令rz）,然后解压<code>tar -zxvf jdk-8u181-linux-x64.tar.gz</code>,更换自己的jdk版本,建立软连接,方便操作<code>ln -s jdk1.8.0_181/ jdk</code>.<br>8.上传hadoop,然后解压<code>tar -zxvf hadoop-2.8.4.tar.gz</code>,建立软连接 <code>ln -s hadoop-2.8.4 hadoop</code>.配置环境变量:<code>vim ~/.bashrc</code>,在最后面加上如下的内容:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/zzti/jdk/ </span><br><span class="line">export HADOOP_HOME=/zzti/hadoop/</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin</span><br></pre></td></tr></table></figure></p><p>然后退出保存,执行<code>source ~/.bashrc</code>,使其生效,然后运行<code>java -version</code>,看到jdk的版本即为成功.<br>9.配置hadoop:<code>cd /zzti/hadoop/etc/hadoop</code>,在core-site.xml的两个configure标签添加如下内容:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/zzti/data/hadoop_tmp&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://zzti:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;io.file.buffer.size&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;4096&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;zzti:2181&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br></pre></td></tr></table></figure></p><p>保存退出后,继续编辑hdfs-site.xml,<code>vim hdfs-site.xml</code>,在configure标签之间添加如下内容:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.nameservices&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;zzti-cluster&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;zzti:50090&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:///zzti/data/hadoop/name&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:///zzti/data/hadoop/data&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br></pre></td></tr></table></figure></p><p>保存退出.配置mapred-site.xml,因为只有<code>mapred-site.xml.template</code>,先cp一下<code>cp mapred-site.xml.template mapred-site.xml</code>,然后再编辑<code>vim mapred-site.xml</code>,然后老规矩,添加如下内容:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobtracker.http.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;zzti:50030&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;zzti:10020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;zzti:19888&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br></pre></td></tr></table></figure></p><p>保存退出.编辑yarn-site.xml,<code>vim yarn-site.xml</code>,老规矩添加入下内容:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br></pre></td></tr></table></figure></p><p>到此,配置结束.<br>10.配置hdfs:创建文件夹<code>mkdir /zzti/data/hadoop/name /zzti/data/hadoop/data</code>,<code>echo zzti &gt; slaves</code> <code>cd /zzti/hadoop/bin</code> <code>./hadoop namenode –format</code> <code>cd /zzti/hadoop/sbin</code> <code>./start-all.sh</code>,完成后,输入<code>jps</code>,查看是否出现NameNode,NodeManager,DataNode,ResourceManager,Jps,出现这些即为配置成功.<code>cd /zzti/hadoop/bin</code> 创建hdfs文件夹<code>hadoop fs -mkdir /data</code>,查看是否创建成功<code>hadoop fs -ls /</code>,出现data目录即为创建成功.<br>11.功能测试:就自行进行,在此不做演示.<br><strong>hadoop安装完成</strong><br>下面进行spark的安装<br><strong>依然在zzti用户下操作</strong><br>1.创建目录:<code>mkdir libs</code>,进入libs,<code>cd libs</code>,上传spark和scala,解压spark<code>tar -zxvf spark-2.4.0-bin-without-hadoop.tgz</code>,建立软连接<code>ln -s spark-2.4.0-bin-without-hadoop spark</code>,解压scala<code>tar -zxvf scala-2.11.6.tgz</code>,创建软连接<code>ln -s scala-2.11.6 scala</code>.<br>2.配置环境变量<code>vim ~/.bashrc</code>,添加如下内容:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_HOME=/zzti/libs/spark</span><br><span class="line">export SCALA_HOME=/zzti/libs/scala</span><br></pre></td></tr></table></figure></p><p>在PATH后面添加<code>:$SCALA_HOME/bin:$SPARK_HOME/bin</code>,保存退出.<code>source ~/.bashrc</code>使其生效.<br>3.配置spark,在spark的conf的目录下,<code>mv slaves.template slaves</code>,<code>echo zzti &gt; slaves</code> <code>mv spark-env.sh.template spark-env.sh</code>,编辑spark-env.sh <code>vim spark-env.sh</code>,在文档最后加入如下内容:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line">export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line">export SPARK_LIBARY_PATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$HADOOP_HOME/lib/native</span><br><span class="line">export SPARK_CLASSPATH=$(/zzti/hadoop/bin/hadoop classpath)</span><br><span class="line">export SPARK_DIST_CLASSPATH=$(/zzti/hadoop/bin/hadoop classpath)</span><br><span class="line">export SPARK_EXECUTOR_INSTANCES=1</span><br><span class="line">export SPARK_EXECUTOR_CORES=1</span><br><span class="line">export SPARK_DRIVER_MEMORY=512M</span><br><span class="line">export SPARK_EXECUTOR_MEMORY=512M</span><br><span class="line">export SPARK_MASTER_HOST=zzti</span><br><span class="line">export SPARK_MASTER_WEBUI_PORT=28686</span><br><span class="line">export SPARK_LOCAL_DIRS=/data/zzti/data/spark/local</span><br><span class="line">export SPARK_WORKER_DIR=/data/zzti/data/spark/work</span><br><span class="line">export SPARK_LOG_DIR=/data/zzti/logs/spark</span><br></pre></td></tr></table></figure></p><p>保存退出.<br>4.启动spark<code>bash /zzti/libs/spark/sbin/start-all.sh</code>,然后输入<code>spark-shell</code>,即可进入操作.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前言:大数据学生一枚,hadoop是基础,spark是进阶,到了学spark的时候了,又得重新配置一台服务器了.正好借这次机会,把配置步骤记下来,省得以后再翻老班发的PPT.&lt;br&gt;环境:Centos7 64位,JDK1.8 hadoop2.8.4,scala2.11.6,spark2.4.0
    
    </summary>
    
      <category term="Linux" scheme="https://mayi21.github.io/categories/Linux/"/>
    
    
      <category term="hadoop" scheme="https://mayi21.github.io/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>创建支持ssh的镜像服务</title>
    <link href="https://mayi21.github.io/2019/03/11/%E5%88%9B%E5%BB%BA%E6%94%AF%E6%8C%81ssh%E6%9C%8D%E5%8A%A1%E7%9A%84%E9%95%9C%E5%83%8F/"/>
    <id>https://mayi21.github.io/2019/03/11/创建支持ssh服务的镜像/</id>
    <published>2019-03-11T11:18:14.028Z</published>
    <updated>2019-03-23T14:24:03.576Z</updated>
    
    <content type="html"><![CDATA[<p>1.先安装docker,这里不再赘述<br>2.使用ubuntu镜像来创建容器:<code>docker pull ubuntu</code>,然后启动容器:<code>docker run -it ubuntu /bin/bash</code><a id="more"></a><br>3.更新一下软件源:<code>apt-get update</code>  <code>apt-get upgrade</code>,然后进行安装<code>apt-get install openssh-server</code><br>4.要是想正常启动服务,需要目录/var/run/sshd存在,手动创建,并启动服务:<code>mkdir -p /var/run/sshd</code>  <code>/usr/sbin/sshd -D &amp;</code><br>5.查看容器的22端口:<code>netstat -tunlp</code>,如果提示命令没有安装,则选择安装:<code>apt-get install net-tools</code>,然后在执行命令就行了.此时端口应该是处于监听状态.<br>6.在root用户目录下创建.ssh目录,并复制需要登录的公钥信息（一般在本地主机用户目录下的.ssh/id_rsa.pub）,先创建文件夹:<code>mkdir root/.ssh</code>,然后复制公钥信息到authorized_keys中,<code>vim /root/.ssh/authorized_keys</code><br>7.创建自启动ssh服务的可执行文件run.sh,并添加可执行权限:<code>vim /run.sh</code>  <code>chmod +x run.sh</code>,run.sh脚本的内容为:<br><code>#!/bin/bash</code><br><code>/usr/sbin/sshd -D</code><br>8.使用<code>Ctrl + p + q</code>退出但并不关闭容器.<br>9.用docker commit命令保存为一个新的sshd:ubuntu镜像:<br><code>docker commit &lt;Image Id&gt; sshd:ubuntu</code>,使用<code>docker images</code>查看本地的镜像<br>10.启动容器,并添加映射端口10022 -&gt; 22,其中10022是宿主机的端口,22是容器的SSH服务监听端口:<code>docker run -p 10022:22 -d sshd:ubuntu /run.sh</code>,启动成功后,在宿主机上看到容器的运行的详细信息.<br>11.在宿主机（192.168.1.200）上或其他主机,通过ssh服务访问10022端口来登录容器:<code>ssh 192.168.1.200 -p 10022</code><br>参考:Docker技术入门与实践</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1.先安装docker,这里不再赘述&lt;br&gt;2.使用ubuntu镜像来创建容器:&lt;code&gt;docker pull ubuntu&lt;/code&gt;,然后启动容器:&lt;code&gt;docker run -it ubuntu /bin/bash&lt;/code&gt;
    
    </summary>
    
      <category term="docker" scheme="https://mayi21.github.io/categories/docker/"/>
    
    
      <category term="docker" scheme="https://mayi21.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>利用Tensorflow构建自定义的图片分类器</title>
    <link href="https://mayi21.github.io/2019/02/17/Tensorflow%E6%9E%84%E5%BB%BA%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E5%99%A8/"/>
    <id>https://mayi21.github.io/2019/02/17/Tensorflow构建自定义的图片分类器/</id>
    <published>2019-02-17T10:24:22.525Z</published>
    <updated>2019-03-23T14:23:24.441Z</updated>
    
    <content type="html"><![CDATA[<h3 id="操作环境-Ubuntu-18-04-Tensorflow-1-7"><a href="#操作环境-Ubuntu-18-04-Tensorflow-1-7" class="headerlink" title="操作环境:Ubuntu 18.04 Tensorflow 1.7"></a>操作环境:Ubuntu 18.04 Tensorflow 1.7</h3><p>1.安装配置Tensorflow或者利用Docker.Tensorflow的安装这里不予多介绍,主要介绍一下Docker的用法.<br>Docker的操作命令如下: <a id="more"></a><br><code>docker pull tensorflow/tensorflow</code><br><code>#指定宿主机的目录和容器的目录,实现文件共享</code><br><code>docker run -it -v /root:/root tensorflow/tensorflow</code><br>2.克隆Git<br><code>git clone https://github.com/googlecodelabs/tensorflow-for-poets-2</code><br><code>cd tensorflow-for-poets-2</code><br>3.准备训练数据,将这些数据存放在tf_files文件夹下,这些数据要求:这个tf_files/&lt;自定义文件夹&gt;下面是子文件夹,子文件夹下面是同种类型的图片(如果是人,那该文件下就是这个人的所有图片,子文件夹的名字就是这些图片的名字)<br>4.准备训练数据.定义图片的大小,<br><code>export IMAGE_SIZE=224</code><br><code>export ARCHITECTURE=&quot;mobilenet_0.50_${IMAGE_SIZE}&quot;</code><br>开始训练.<br><code>python -m scripts.retrain --bottleneck_dir=tf_files/bottlenecks --how_many_training_steps=500 --model_dir=tf_files/models/ --summaries_dir=tf_files/training_summaries/&quot;${ARCHITECTURE}&quot; --output_graph=tf_files/retrained_graph.pb --output_labels=tf_files/retrained_labels.txt --architecture=&quot;${ARCHITECTURE}&quot; --image_dir=tf_files/&lt;自定义的文件夹&gt;</code><br>–how_many_training_steps=500 这个是训练多少步.这里可以不用这个命令,使用默认步数4000步<br>5.验证数据.<br><code>python -m scripts.label_image --graph=tf_files/retrained_graph.pb --image=&lt;自己想检测的图片&gt;</code><br>然后下面就会出现验证的结果.</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;操作环境-Ubuntu-18-04-Tensorflow-1-7&quot;&gt;&lt;a href=&quot;#操作环境-Ubuntu-18-04-Tensorflow-1-7&quot; class=&quot;headerlink&quot; title=&quot;操作环境:Ubuntu 18.04 Tensorflow 1.7&quot;&gt;&lt;/a&gt;操作环境:Ubuntu 18.04 Tensorflow 1.7&lt;/h3&gt;&lt;p&gt;1.安装配置Tensorflow或者利用Docker.Tensorflow的安装这里不予多介绍,主要介绍一下Docker的用法.&lt;br&gt;Docker的操作命令如下:
    
    </summary>
    
      <category term="python" scheme="https://mayi21.github.io/categories/python/"/>
    
    
      <category term="Tensorflow" scheme="https://mayi21.github.io/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>idea开发hadoop</title>
    <link href="https://mayi21.github.io/2018/12/03/idea%E5%BC%80%E5%8F%91hadoop/"/>
    <id>https://mayi21.github.io/2018/12/03/idea开发hadoop/</id>
    <published>2018-12-03T08:54:53.797Z</published>
    <updated>2019-03-24T04:02:39.560Z</updated>
    
    <content type="html"><![CDATA[<p>下载idea和hadopp源码包<br>新建一个java project,用于hadoop开发,同时需要导入hadoop的相关依赖包<br>java project建成后,点击project structure,进入Dependencies,点击右边的+,点击JARs or<a id="more"></a><br>directories…,添加依赖,这些依赖都可以在share/hadoop下面找到<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">common</span><br><span class="line">hdfs</span><br><span class="line">mapreduce</span><br><span class="line">yarn </span><br><span class="line">common/lib</span><br></pre></td></tr></table></figure></p><p>然后是Artifacts,点击+,添加JAR&gt;Empty,名称自定义,然后点击+,点击Module output,在弹出的对话框选择当前的项目,点击保存.<br>接着新建一个Application,Edit Configurations,点击+,新建一个Application,在Main Class中填入<br><code>org.apache.hadoop.util.RunJar</code><br>Working directory当然是选择当前项目的目录了, Program arguments  这个是设置默认参数的会在程序执行的时候传递进去<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/home/edmond/workspace/IdeaProjects/Hadoop/HadoopWordCount/out/artifacts/wordcount/wordcount.jar</span><br><span class="line">com.company.Main</span><br><span class="line">input</span><br><span class="line">output</span><br></pre></td></tr></table></figure></p><p>第一个是jar包所在的位置<br>第二个是Main函数所在的类<br>第三四两个参数是由自己决定的（这两个参数会作为args[0]和args[1]传入）<br>点击ok保存.<br>自己写mapper和reducer测试吧！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;下载idea和hadopp源码包&lt;br&gt;新建一个java project,用于hadoop开发,同时需要导入hadoop的相关依赖包&lt;br&gt;java project建成后,点击project structure,进入Dependencies,点击右边的+,点击JARs or
    
    </summary>
    
      <category term="Java" scheme="https://mayi21.github.io/categories/Java/"/>
    
    
      <category term="hadoop" scheme="https://mayi21.github.io/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>配置远程访问jupyter</title>
    <link href="https://mayi21.github.io/2018/07/21/%E9%85%8D%E7%BD%AE%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AEjupyter/"/>
    <id>https://mayi21.github.io/2018/07/21/配置远程访问jupyter/</id>
    <published>2018-07-21T09:58:08.241Z</published>
    <updated>2019-03-24T04:04:07.145Z</updated>
    
    <content type="html"><![CDATA[<p>##前言:jupyternotebook是一款在线的python编程环境.交互式的编程环境,支持四十多种语言.对于爬虫和分析数据,这样的操作十分方便.众所周知:Jupyter是一个WEB应用.本篇文章教大家在服务器端部署Jupyter并远程访问.<br>1.服务器环境:大家可以自行在网上购买服务器,本篇文章使用的服务器环境是Ubuntu 16.04.1 LTS x86_64.<br>2.安全规则:添加安全规则,开放8888端口,记得同时开放22端口,以便在putty,xshell等软件上登陆;然后关联服务器实例.<a id="more"></a><br>3.使用wget下载anaconda,在<a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/" target="_blank" rel="noopener">清华大学开源软件镜像站</a>寻找相应的版本,复制下载链接,然后wget便可.<br>4.使用sh Anaconda3-5.1.0-Linux-x86_64.sh安装Anaconda,安装一路yes便可,安装完成后重启终端,尝试<code>conda</code>.<br>5.输入<code>jupyter notebook --generate-config</code>,生成默认的jupyter配置文件<br>6.安装ipython,<code>sudo apt-get install ipython3</code>.<br>7.启动ipython<code>ipthon</code>,输入<code>from notebook.auth import passwd</code>回车,输入<code>passwd()</code>,此时要求输入密码,这个密码为登陆服务器端jupyter的密码.两次输入后得到密钥(以sha开头的).<br>8.<code>vim /home/ubuntu/.jupyter/jupyter_notebook_config.py</code><br>9.在<code># Configuration file for jupyter-notebook.</code>后另起一行添加<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">c.NotebookApp.ip=&apos;*&apos;</span><br><span class="line">c.NotebookApp.password = u&apos;密钥&apos;</span><br><span class="line">c.NotebookApp.open_browser = False</span><br><span class="line">c.NotebookApp.port =8888</span><br></pre></td></tr></table></figure></p><p>保存退出.<br>10.安装自动补码,在jupyter notebook未运行的情况下操作<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install jupyter_contrib_nbextensions</span><br><span class="line">jupyter contrib nbextension install --user --skip-running-check</span><br></pre></td></tr></table></figure></p><p>然后在 Clusters旁边多出 Nbextensions点击进去,找到 Hinterland,打勾即可<br>10.运行Jupyter Notebook <code>jupyter notebook</code>,后台运行<code>nohup jupyter notebook --allow-root &gt; jnotebook.out 2&gt;&amp;1 &amp;</code><br>11.在本地电脑浏览器上输入[<a href="http://you" target="_blank" rel="noopener">http://you</a> server ip:8888]<br>12.输入上面设置的密码.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;##前言:jupyternotebook是一款在线的python编程环境.交互式的编程环境,支持四十多种语言.对于爬虫和分析数据,这样的操作十分方便.众所周知:Jupyter是一个WEB应用.本篇文章教大家在服务器端部署Jupyter并远程访问.&lt;br&gt;1.服务器环境:大家可以自行在网上购买服务器,本篇文章使用的服务器环境是Ubuntu 16.04.1 LTS x86_64.&lt;br&gt;2.安全规则:添加安全规则,开放8888端口,记得同时开放22端口,以便在putty,xshell等软件上登陆;然后关联服务器实例.
    
    </summary>
    
      <category term="python" scheme="https://mayi21.github.io/categories/python/"/>
    
    
      <category term="python开发" scheme="https://mayi21.github.io/tags/python%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>hexo建站</title>
    <link href="https://mayi21.github.io/2018/06/26/hexo%E5%BB%BA%E7%AB%99%E7%AC%AC%E4%B8%80%E6%AD%A5-1/"/>
    <id>https://mayi21.github.io/2018/06/26/hexo建站第一步-1/</id>
    <published>2018-06-26T14:16:33.965Z</published>
    <updated>2019-03-23T14:21:46.239Z</updated>
    
    <content type="html"><![CDATA[<p>1.下载git和node.js<br>2.打开GitBash<br>3.下载并安装hexo<a id="more"></a><br>4.新建一个文件夹,hexo inital先初始化一下该目录<br>5.自己找合适的主题<br>6.在git bash中运行<code>hexo g</code>生成本地文件,<code>hexo d</code>将其发布到github上面<br>7.在source/_posts/下面写文章,我用的是MarkdownPad2,只要熟悉一下语法就行    </p><hr><p>今天主要谈一下绑定域名<br>1.申请域名,闲着没事干,免费申请了一年的域名,<a href="http://www.freenom.com" target="_blank" rel="noopener">freenom</a><br>2.介绍一下主要怎么操作,先“寻找一个新的免费域名”,点击“检查可用性”,会出来一些免费的域名.<br>3.自己选其所好,找到一个加入购物车<br>4.去购物车结帐,都是不要钱的.点击“继续”,<br>5.这时候应该要求填写个人信息,这里提供<a href="https://www.fakenamegenerator.com/" target="_blank" rel="noopener">随机生成身份</a>.选择想要的信息.<br>6.邮箱要填自己的,然后邮箱收件箱会收到一封邮件,要求你注册一下.<br>7.你注册好后,在上面网站继续登录.你就会发现你有这个域名了（如果没有,你再搜一下,重新添加进你的购物车）<br>8.有域名后,在你的博客source目录下,添加名字为<code>CNAME</code>（注意没有后缀）的文件,里面的内容就是你的域名.然后<code>hexo g</code>和<code>hexo d</code>下.继而进入DNSPOD把自己的域名解析一下.<br>9.主要先登录,qq,微信都可以.登录上,进入管理控制台,添加自己的域名到域名解析.添加记录的每一项,系统都会提示代表意思,这里主要解释记录类型<br>A记录:地址记录,用来指定域名的IP地址<br>CNAME记录:如果需要将域名指向另一个域名,再由另一个域名提供IP地址,就需要添加CNAME记录<br>NS记录:域名服务器记录,如果需要把子域名交给其他DNS服务商解析,就需要添加NS记录<br>上面的NS记录是系统默认添加的.<br>A记录就是指向对应IP地址,这里的<code>192.30.252.153</code>和<code>192.30.252.154</code>是github的服务器IP地址.<br>CNAME记录这里可填可不填,因为A记录已经将<code>mayi21.tk</code>和<code>mayi21.github.io</code>的域名统一为一个IP地址了<br>有一种情况就是为了提高访问速度,要区分国内国外不同用户使用不同的网站进行重定向需要添加对应的CNAME记录.<br>10.然后继续输入你的<code>name.github.io</code>,就会跳转到你绑定的域名中.</p><hr><p>google收录<br>1.打开<a href="https://www.google.com/webmasters" target="_blank" rel="noopener">谷歌搜索引擎验证</a><br>2.输入自己的博客地址,添加属性,我用的是域名服务商验证,把google提供的文本,按照提示添加进入DNSPOD就行了<br>3.点击验证,应该就可以成功了</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1.下载git和node.js&lt;br&gt;2.打开GitBash&lt;br&gt;3.下载并安装hexo
    
    </summary>
    
      <category term="建站" scheme="https://mayi21.github.io/categories/%E5%BB%BA%E7%AB%99/"/>
    
    
      <category term="hexo建站" scheme="https://mayi21.github.io/tags/hexo%E5%BB%BA%E7%AB%99/"/>
    
  </entry>
  
</feed>
