{"meta":{"title":"wiki","subtitle":"my hearts will go on!","description":"坚持不懈的坚持","author":"Mayi21","url":"https://mayi21.github.io"},"pages":[{"title":"","date":"2018-09-21T13:20:28.484Z","updated":"2018-08-31T06:34:42.303Z","comments":true,"path":"404.html","permalink":"https://mayi21.github.io/404.html","excerpt":"","text":"404页面 *{margin:0;padding:0;outline:none;font-family:\\5FAE\\8F6F\\96C5\\9ED1,宋体;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;-khtml-user-select:none;user-select:none;cursor:default;font-weight:lighter;} .center{margin:0 auto;} .whole{width:100%;height:100%;line-height:100%;position:fixed;bottom:0;left:0;z-index:-1000;overflow:hidden;} .whole img{width:100%;height:100%;} .mask{width:100%;height:100%;position:absolute;top:0;left:0;background:#000;opacity:0.6;filter:alpha(opacity=60);} .b{width:100%;text-align:center;height:400px;position:absolute;top:50%;margin-top:-230px}.a{width:150px;height:50px;margin-top:30px}.a a{display:block;float:left;width:150px;height:50px;background:#fff;text-align:center;line-height:50px;font-size:18px;border-radius:25px;color:#333}.a a:hover{color:#000;box-shadow:#fff 0 0 20px} p{color:#fff;margin-top:40px;font-size:24px;} #num{margin:0 5px;font-weight:bold;} .plan{color: black;background: white;font-size: 30px; margin-top: 20px;} .plan:hover{color: white;background: black;font-size: 30px;} #gg { position: absolute; width: 654px; height: 470px; left: 50%; top: 50%; margin-left: -377px; margin-top: -235px; }"},{"title":"","date":"2018-09-21T13:21:42.644Z","updated":"2018-09-21T13:11:57.692Z","comments":true,"path":"googlea6b07bee95a649f7.html","permalink":"https://mayi21.github.io/googlea6b07bee95a649f7.html","excerpt":"","text":"google-site-verification: googlea6b07bee95a649f7.html"},{"title":"","date":"2018-08-31T05:51:21.039Z","updated":"2018-08-31T05:51:21.039Z","comments":true,"path":"categories/index.html","permalink":"https://mayi21.github.io/categories/index.html","excerpt":"","text":"title: 文章分类date: 2018-08-24 14:47:21type: “categories”"}],"posts":[{"title":"Spark-1","slug":"Spark","date":"2019-06-06T11:40:08.745Z","updated":"2019-06-06T12:28:48.911Z","comments":true,"path":"2019/06/06/Spark/","link":"","permalink":"https://mayi21.github.io/2019/06/06/Spark/","excerpt":"","text":"1.分布式数据的容错性处理较为常用的方法有检查节点和更新记录。检查节点的方法是对每个节点逐个进行检测，随时查询每个节点的运行情况。好处是便于主节点随时了解人物的真实数据运行情况，坏处就是资源消耗非常大，而且一旦出现问题，需要将数据在不同节点中搬运，反而更加耗费时间从而极大地拉低执行效率。更新记录指的是运行的主节点并不总是查询每个分节点的运行状态，而是将相同的数据存放在不同的节点上面（一般不超过三个），各个工作节点按固定的周期更新在主节点中的运行记录，如果在一定的时间内主节点查询到的数据的更新状态超时或者有异常，则在存储相同数据的不同节点上重新启动数据计算工作。缺点在于如果数据量过大，更新数据和重新启动运行任务的资源耗费也相当大。2.Spark架构Driver:运行Application的main函数并且创建SparkContext；Client:用户提交作业的客户端；Worker:集群中任何可以运行Application代码的节点，运行一个或多个Executor的进程；Executor:运行在worker的Task执行器，Executor启动线程池运行Task，并且负责将数据存在磁盘或者内存中。每个Application都会申请各自的Executor来处理任务；SparkContext:整个应用的上下文，控制应用的声明周期；RDD:Spark的基本计算单元，一组rdd形成执行的有向无环图RDD Graph；DAG Scheduler:根据job构建基于stage的dag工作流，并提交stage给taskscheduler；TaskScheduler:将Task分发给Executor执行；SparkEnv:线程级别的上下文，储存运行时的重要组件的引用。 华丽分割线 小测试中1.scala中的nothing是所有类的子类。2.ClusterManager是主节点。3.scala反编译后可读可执行，是字节码文件。","categories":[{"name":"Spark","slug":"Spark","permalink":"https://mayi21.github.io/categories/Spark/"}],"tags":[{"name":"Spark开发","slug":"Spark开发","permalink":"https://mayi21.github.io/tags/Spark开发/"}]},{"title":"Python-1","slug":"Python-1","date":"2019-06-06T10:28:46.040Z","updated":"2019-06-06T12:49:17.251Z","comments":true,"path":"2019/06/06/Python-1/","link":"","permalink":"https://mayi21.github.io/2019/06/06/Python-1/","excerpt":"","text":"Q.调用方法带括号和不带括号的区别？A.带括号返回的是计算结果，是一个值；不带括号返回的是这个函数体。 华丽分割线 1.list去重：List(Set(List)) 1.math库的高等特殊函数函数 | 数学表示 | 描述 | :-: | -math.erf(x) | $\\frac{2}{\\sqrt{\\pi}}\\lmoustache_{0}^{x}e^{-t^{2}}dt$ | 高斯误差，应用于概率论，统计学等领域math.erfc(x) | $\\frac{2}{\\sqrt{\\pi}}\\lmoustache_{0}^{\\infty}e^{-t^{2}}dt$ | 余补高斯误差函数，math.erfc(x)=1-math.erf(x)math.gamma(x) | $\\lmoustache_{0}^{\\infty}x^{t-1}e^{-x}dx$ | 伽玛函数，也叫欧拉第二积分函数math.lgamma(x) | ln(gamma(x)) | 伽马函数的自然对数","categories":[{"name":"Python","slug":"Python","permalink":"https://mayi21.github.io/categories/Python/"}],"tags":[{"name":"Python开发","slug":"Python开发","permalink":"https://mayi21.github.io/tags/Python开发/"}]},{"title":"java笔记-1","slug":"Java-1","date":"2019-05-22T10:47:49.286Z","updated":"2019-06-07T15:12:12.050Z","comments":true,"path":"2019/05/22/Java-1/","link":"","permalink":"https://mayi21.github.io/2019/05/22/Java-1/","excerpt":"Q.String,StringBuffer和StringBuilder的区别A.String是字符串常量，而StringBuilder和StringBuffer是字符串变量；由String创建的字符内容不可改变，而由StringBuffer和StringBuilder创建的字符内容可以改变；StringBuffer是线程安全的，StringBuilder是线程不安全的，但在可能的情况下先建议使用此，因为不需要考虑同步问题，","text":"Q.String,StringBuffer和StringBuilder的区别A.String是字符串常量，而StringBuilder和StringBuffer是字符串变量；由String创建的字符内容不可改变，而由StringBuffer和StringBuilder创建的字符内容可以改变；StringBuffer是线程安全的，StringBuilder是线程不安全的，但在可能的情况下先建议使用此，因为不需要考虑同步问题，所以会更快，String是线程安全的，但是线程安全主要是针对那些需要维护内部状态改变的类，而String是不可以改变的，所有操作都不会对它进行修改，所以一般不讨论String的线程安全问题。Q.静态代码块和主函数？A.静态代码块先执行，然后执行主函数；静态代码块中前面的先执行，后面的后执行。Q.HashMap怎么put()和get()元素？A.HashMap的put实现:根据key的hashcode计算key值的hash值，根据hash值插入到表中的位置，如果该位置有元素，便以链表的方式存储，新加入的元素放在链表头；最先加入的元素放在链表尾。HashMap的get实现:根据key的hashcode值计算hash值，然后找到在表中的位置，然后通过equals方法，逐个寻找在链表中的位置。Q.HashSet怎样保证所存元素单一？A.根据hash码和equals方法：如果hash码值不相同，则说明元素不存在，就存入；如果hash码值相同则进行equals方法判断是否为同一对象，如果判断相等，就不存，如果判断不等，就存入。(hash值是根据hashcode计算到的)。HashSet的put方法源码为： 123public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125; 其中map为HashMap，PRESENT是final Object。HashMap的put源码为： public V put(K key, V value) { return putVal(hash(key), key, value, false, true); } /** * Implements Map.put and related methods * * @param hash key的hash值 * @param key 就是key * @param value 就是value * @param onlyIfAbsent if true, don&apos;t change existing value(如果为true，就不能改变已经存在的值) * @param evict if false, the table is in creation mode(如果为false，表就在创造模式下) * @return previous value, or null if none(返回先前的值或者null，如果输入的值为none) */ final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else { Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else { for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; } 华丽的分割线 transient关键字：Java的serialization提供了一种持久化对象实例的机制。当持久化对象时，可能有一个特殊的对象数据成员，我们不想用serialization机制来保存它。为了在一个特定对象的一个域上关闭serialization，可以在这个域前加上关键字transient。当一个对象被序列化的时候，transient型变量的值不包括在序列化的表示中，然而非transient型的变量是被包括进去的。","categories":[{"name":"Java","slug":"Java","permalink":"https://mayi21.github.io/categories/Java/"}],"tags":[{"name":"Java开发","slug":"Java开发","permalink":"https://mayi21.github.io/tags/Java开发/"}]},{"title":"Centos配置hadoop和spark","slug":"Linux配置hadoop与spark","date":"2019-03-23T13:05:17.152Z","updated":"2019-03-24T03:59:35.673Z","comments":true,"path":"2019/03/23/Linux配置hadoop与spark/","link":"","permalink":"https://mayi21.github.io/2019/03/23/Linux配置hadoop与spark/","excerpt":"前言:大数据学生一枚,hadoop是基础,spark是进阶,到了学spark的时候了,又得重新配置一台服务器了.正好借这次机会,把配置步骤记下来,省得以后再翻老班发的PPT.环境:Centos7 64位,JDK1.8 hadoop2.8.4,scala2.11.6,spark2.4.0","text":"前言:大数据学生一枚,hadoop是基础,spark是进阶,到了学spark的时候了,又得重新配置一台服务器了.正好借这次机会,把配置步骤记下来,省得以后再翻老班发的PPT.环境:Centos7 64位,JDK1.8 hadoop2.8.4,scala2.11.6,spark2.4.01.先安装通用工具:yum install lrzsz.安装完成后在Xshell中使用rz命令可选择上传文件,十分方便.2.配置网关:vim /etc/sysconfig/network,应该是空的,添加内容如下HOSTNAME=zzti GATEWAY=192.168.17.2,网关设置也可能不是,自己可能要改一下.3.修改主机名称:vim /etc/hosts,在最前面添加127.0.0.1 zzti. 然后关闭防火墙systemctl stop firewalld 查看防火墙状态的命令:systemctl status firewalld.4.此时重启一下,倒杯水,准备下面一番苦干吧.reboot.5.创建工作目录:mkdir -p /data/zzti ln -s /data/zzti/ /zzti useradd zzti chown zzti:zzti /zzti.6.配置免密登录:ssh-keygen,一路回车到底,然后执行cat ~/.ssh/id_rsa.pub &gt; ~/.ssh/authorized_keys chmod 644 ~/.ssh/authorized_keys mkdir /home/zzti/.ssh cp ~/.ssh/* /home/zzti/.ssh/ chown -R zzti:zzti /home/zzti/.ssh chmod 700 /home/zzti/.ssh,此过程应该没什么错误.下面操作在zzti用户下完成 su zzti7.上传JDK（此处省略无数字,自己找jdk,上传命令rz）,然后解压tar -zxvf jdk-8u181-linux-x64.tar.gz,更换自己的jdk版本,建立软连接,方便操作ln -s jdk1.8.0_181/ jdk.8.上传hadoop,然后解压tar -zxvf hadoop-2.8.4.tar.gz,建立软连接 ln -s hadoop-2.8.4 hadoop.配置环境变量:vim ~/.bashrc,在最后面加上如下的内容:123export JAVA_HOME=/zzti/jdk/ export HADOOP_HOME=/zzti/hadoop/export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin 然后退出保存,执行source ~/.bashrc,使其生效,然后运行java -version,看到jdk的版本即为成功.9.配置hadoop:cd /zzti/hadoop/etc/hadoop,在core-site.xml的两个configure标签添加如下内容:12345678910111213141516&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/zzti/data/hadoop_tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://zzti:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;zzti:2181&lt;/value&gt; &lt;/property&gt; 保存退出后,继续编辑hdfs-site.xml,vim hdfs-site.xml,在configure标签之间添加如下内容:123456789101112131415161718192021222324&lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;zzti-cluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;zzti:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///zzti/data/hadoop/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///zzti/data/hadoop/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; 保存退出.配置mapred-site.xml,因为只有mapred-site.xml.template,先cp一下cp mapred-site.xml.template mapred-site.xml,然后再编辑vim mapred-site.xml,然后老规矩,添加如下内容:12345678910111213141516&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobtracker.http.address&lt;/name&gt; &lt;value&gt;zzti:50030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;zzti:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;zzti:19888&lt;/value&gt; &lt;/property&gt; 保存退出.编辑yarn-site.xml,vim yarn-site.xml,老规矩添加入下内容:1234&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 到此,配置结束.10.配置hdfs:创建文件夹mkdir /zzti/data/hadoop/name /zzti/data/hadoop/data,echo zzti &gt; slaves cd /zzti/hadoop/bin ./hadoop namenode –format cd /zzti/hadoop/sbin ./start-all.sh,完成后,输入jps,查看是否出现NameNode,NodeManager,DataNode,ResourceManager,Jps,出现这些即为配置成功.cd /zzti/hadoop/bin 创建hdfs文件夹hadoop fs -mkdir /data,查看是否创建成功hadoop fs -ls /,出现data目录即为创建成功.11.功能测试:就自行进行,在此不做演示.hadoop安装完成下面进行spark的安装依然在zzti用户下操作1.创建目录:mkdir libs,进入libs,cd libs,上传spark和scala,解压sparktar -zxvf spark-2.4.0-bin-without-hadoop.tgz,建立软连接ln -s spark-2.4.0-bin-without-hadoop spark,解压scalatar -zxvf scala-2.11.6.tgz,创建软连接ln -s scala-2.11.6 scala.2.配置环境变量vim ~/.bashrc,添加如下内容:12export SPARK_HOME=/zzti/libs/sparkexport SCALA_HOME=/zzti/libs/scala 在PATH后面添加:$SCALA_HOME/bin:$SPARK_HOME/bin,保存退出.source ~/.bashrc使其生效.3.配置spark,在spark的conf的目录下,mv slaves.template slaves,echo zzti &gt; slaves mv spark-env.sh.template spark-env.sh,编辑spark-env.sh vim spark-env.sh,在文档最后加入如下内容:1234567891011121314export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport YARN_CONF_DIR=$HADOOP_HOME/etc/hadoopexport SPARK_LIBARY_PATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$HADOOP_HOME/lib/nativeexport SPARK_CLASSPATH=$(/zzti/hadoop/bin/hadoop classpath)export SPARK_DIST_CLASSPATH=$(/zzti/hadoop/bin/hadoop classpath)export SPARK_EXECUTOR_INSTANCES=1export SPARK_EXECUTOR_CORES=1export SPARK_DRIVER_MEMORY=512Mexport SPARK_EXECUTOR_MEMORY=512Mexport SPARK_MASTER_HOST=zztiexport SPARK_MASTER_WEBUI_PORT=28686export SPARK_LOCAL_DIRS=/data/zzti/data/spark/localexport SPARK_WORKER_DIR=/data/zzti/data/spark/workexport SPARK_LOG_DIR=/data/zzti/logs/spark 保存退出.4.启动sparkbash /zzti/libs/spark/sbin/start-all.sh,然后输入spark-shell,即可进入操作.","categories":[{"name":"Linux","slug":"Linux","permalink":"https://mayi21.github.io/categories/Linux/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://mayi21.github.io/tags/hadoop/"}]},{"title":"创建支持ssh的镜像服务","slug":"创建支持ssh服务的镜像","date":"2019-03-11T11:18:14.028Z","updated":"2019-03-23T14:24:03.576Z","comments":true,"path":"2019/03/11/创建支持ssh服务的镜像/","link":"","permalink":"https://mayi21.github.io/2019/03/11/创建支持ssh服务的镜像/","excerpt":"1.先安装docker,这里不再赘述2.使用ubuntu镜像来创建容器:docker pull ubuntu,然后启动容器:docker run -it ubuntu /bin/bash","text":"1.先安装docker,这里不再赘述2.使用ubuntu镜像来创建容器:docker pull ubuntu,然后启动容器:docker run -it ubuntu /bin/bash3.更新一下软件源:apt-get update apt-get upgrade,然后进行安装apt-get install openssh-server4.要是想正常启动服务,需要目录/var/run/sshd存在,手动创建,并启动服务:mkdir -p /var/run/sshd /usr/sbin/sshd -D &amp;5.查看容器的22端口:netstat -tunlp,如果提示命令没有安装,则选择安装:apt-get install net-tools,然后在执行命令就行了.此时端口应该是处于监听状态.6.在root用户目录下创建.ssh目录,并复制需要登录的公钥信息（一般在本地主机用户目录下的.ssh/id_rsa.pub）,先创建文件夹:mkdir root/.ssh,然后复制公钥信息到authorized_keys中,vim /root/.ssh/authorized_keys7.创建自启动ssh服务的可执行文件run.sh,并添加可执行权限:vim /run.sh chmod +x run.sh,run.sh脚本的内容为:#!/bin/bash/usr/sbin/sshd -D8.使用Ctrl + p + q退出但并不关闭容器.9.用docker commit命令保存为一个新的sshd:ubuntu镜像:docker commit &lt;Image Id&gt; sshd:ubuntu,使用docker images查看本地的镜像10.启动容器,并添加映射端口10022 -&gt; 22,其中10022是宿主机的端口,22是容器的SSH服务监听端口:docker run -p 10022:22 -d sshd:ubuntu /run.sh,启动成功后,在宿主机上看到容器的运行的详细信息.11.在宿主机（192.168.1.200）上或其他主机,通过ssh服务访问10022端口来登录容器:ssh 192.168.1.200 -p 10022参考:Docker技术入门与实践","categories":[{"name":"docker","slug":"docker","permalink":"https://mayi21.github.io/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://mayi21.github.io/tags/docker/"}]},{"title":"利用Tensorflow构建自定义的图片分类器","slug":"Tensorflow构建自定义的图片分类器","date":"2019-02-17T10:24:22.525Z","updated":"2019-03-23T14:23:24.441Z","comments":true,"path":"2019/02/17/Tensorflow构建自定义的图片分类器/","link":"","permalink":"https://mayi21.github.io/2019/02/17/Tensorflow构建自定义的图片分类器/","excerpt":"操作环境:Ubuntu 18.04 Tensorflow 1.71.安装配置Tensorflow或者利用Docker.Tensorflow的安装这里不予多介绍,主要介绍一下Docker的用法.Docker的操作命令如下:","text":"操作环境:Ubuntu 18.04 Tensorflow 1.71.安装配置Tensorflow或者利用Docker.Tensorflow的安装这里不予多介绍,主要介绍一下Docker的用法.Docker的操作命令如下: docker pull tensorflow/tensorflow#指定宿主机的目录和容器的目录,实现文件共享docker run -it -v /root:/root tensorflow/tensorflow2.克隆Gitgit clone https://github.com/googlecodelabs/tensorflow-for-poets-2cd tensorflow-for-poets-23.准备训练数据,将这些数据存放在tf_files文件夹下,这些数据要求:这个tf_files/&lt;自定义文件夹&gt;下面是子文件夹,子文件夹下面是同种类型的图片(如果是人,那该文件下就是这个人的所有图片,子文件夹的名字就是这些图片的名字)4.准备训练数据.定义图片的大小,export IMAGE_SIZE=224export ARCHITECTURE=&quot;mobilenet_0.50_${IMAGE_SIZE}&quot;开始训练.python -m scripts.retrain --bottleneck_dir=tf_files/bottlenecks --how_many_training_steps=500 --model_dir=tf_files/models/ --summaries_dir=tf_files/training_summaries/&quot;${ARCHITECTURE}&quot; --output_graph=tf_files/retrained_graph.pb --output_labels=tf_files/retrained_labels.txt --architecture=&quot;${ARCHITECTURE}&quot; --image_dir=tf_files/&lt;自定义的文件夹&gt;–how_many_training_steps=500 这个是训练多少步.这里可以不用这个命令,使用默认步数4000步5.验证数据.python -m scripts.label_image --graph=tf_files/retrained_graph.pb --image=&lt;自己想检测的图片&gt;然后下面就会出现验证的结果.","categories":[{"name":"python","slug":"python","permalink":"https://mayi21.github.io/categories/python/"}],"tags":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"https://mayi21.github.io/tags/Tensorflow/"}]},{"title":"idea开发hadoop","slug":"idea开发hadoop","date":"2018-12-03T08:54:53.797Z","updated":"2019-03-24T04:02:39.560Z","comments":true,"path":"2018/12/03/idea开发hadoop/","link":"","permalink":"https://mayi21.github.io/2018/12/03/idea开发hadoop/","excerpt":"下载idea和hadopp源码包新建一个java project,用于hadoop开发,同时需要导入hadoop的相关依赖包java project建成后,点击project structure,进入Dependencies,点击右边的+,点击JARs or","text":"下载idea和hadopp源码包新建一个java project,用于hadoop开发,同时需要导入hadoop的相关依赖包java project建成后,点击project structure,进入Dependencies,点击右边的+,点击JARs ordirectories…,添加依赖,这些依赖都可以在share/hadoop下面找到12345commonhdfsmapreduceyarn common/lib 然后是Artifacts,点击+,添加JAR&gt;Empty,名称自定义,然后点击+,点击Module output,在弹出的对话框选择当前的项目,点击保存.接着新建一个Application,Edit Configurations,点击+,新建一个Application,在Main Class中填入org.apache.hadoop.util.RunJarWorking directory当然是选择当前项目的目录了, Program arguments 这个是设置默认参数的会在程序执行的时候传递进去1234/home/edmond/workspace/IdeaProjects/Hadoop/HadoopWordCount/out/artifacts/wordcount/wordcount.jarcom.company.Maininputoutput 第一个是jar包所在的位置第二个是Main函数所在的类第三四两个参数是由自己决定的（这两个参数会作为args[0]和args[1]传入）点击ok保存.自己写mapper和reducer测试吧！","categories":[{"name":"Java","slug":"Java","permalink":"https://mayi21.github.io/categories/Java/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://mayi21.github.io/tags/hadoop/"}]},{"title":"配置远程访问jupyter","slug":"配置远程访问jupyter","date":"2018-07-21T09:58:08.241Z","updated":"2019-03-24T04:04:07.145Z","comments":true,"path":"2018/07/21/配置远程访问jupyter/","link":"","permalink":"https://mayi21.github.io/2018/07/21/配置远程访问jupyter/","excerpt":"##前言:jupyternotebook是一款在线的python编程环境.交互式的编程环境,支持四十多种语言.对于爬虫和分析数据,这样的操作十分方便.众所周知:Jupyter是一个WEB应用.本篇文章教大家在服务器端部署Jupyter并远程访问.1.服务器环境:大家可以自行在网上购买服务器,本篇文章使用的服务器环境是Ubuntu 16.04.1 LTS x86_64.2.安全规则:添加安全规则,开放8888端口,记得同时开放22端口,以便在putty,xshell等软件上登陆;然后关联服务器实例.","text":"##前言:jupyternotebook是一款在线的python编程环境.交互式的编程环境,支持四十多种语言.对于爬虫和分析数据,这样的操作十分方便.众所周知:Jupyter是一个WEB应用.本篇文章教大家在服务器端部署Jupyter并远程访问.1.服务器环境:大家可以自行在网上购买服务器,本篇文章使用的服务器环境是Ubuntu 16.04.1 LTS x86_64.2.安全规则:添加安全规则,开放8888端口,记得同时开放22端口,以便在putty,xshell等软件上登陆;然后关联服务器实例.3.使用wget下载anaconda,在清华大学开源软件镜像站寻找相应的版本,复制下载链接,然后wget便可.4.使用sh Anaconda3-5.1.0-Linux-x86_64.sh安装Anaconda,安装一路yes便可,安装完成后重启终端,尝试conda.5.输入jupyter notebook --generate-config,生成默认的jupyter配置文件6.安装ipython,sudo apt-get install ipython3.7.启动ipythonipthon,输入from notebook.auth import passwd回车,输入passwd(),此时要求输入密码,这个密码为登陆服务器端jupyter的密码.两次输入后得到密钥(以sha开头的).8.vim /home/ubuntu/.jupyter/jupyter_notebook_config.py9.在# Configuration file for jupyter-notebook.后另起一行添加1234c.NotebookApp.ip=&apos;*&apos;c.NotebookApp.password = u&apos;密钥&apos;c.NotebookApp.open_browser = Falsec.NotebookApp.port =8888 保存退出.10.安装自动补码,在jupyter notebook未运行的情况下操作12python -m pip install jupyter_contrib_nbextensionsjupyter contrib nbextension install --user --skip-running-check 然后在 Clusters旁边多出 Nbextensions点击进去,找到 Hinterland,打勾即可10.运行Jupyter Notebook jupyter notebook,后台运行nohup jupyter notebook --allow-root &gt; jnotebook.out 2&gt;&amp;1 &amp;11.在本地电脑浏览器上输入[http://you server ip:8888]12.输入上面设置的密码.","categories":[{"name":"python","slug":"python","permalink":"https://mayi21.github.io/categories/python/"}],"tags":[{"name":"python开发","slug":"python开发","permalink":"https://mayi21.github.io/tags/python开发/"}]},{"title":"hexo建站","slug":"hexo建站第一步-1","date":"2018-06-26T14:16:33.965Z","updated":"2019-03-23T14:21:46.239Z","comments":true,"path":"2018/06/26/hexo建站第一步-1/","link":"","permalink":"https://mayi21.github.io/2018/06/26/hexo建站第一步-1/","excerpt":"1.下载git和node.js2.打开GitBash3.下载并安装hexo","text":"1.下载git和node.js2.打开GitBash3.下载并安装hexo4.新建一个文件夹,hexo inital先初始化一下该目录5.自己找合适的主题6.在git bash中运行hexo g生成本地文件,hexo d将其发布到github上面7.在source/_posts/下面写文章,我用的是MarkdownPad2,只要熟悉一下语法就行 今天主要谈一下绑定域名1.申请域名,闲着没事干,免费申请了一年的域名,freenom2.介绍一下主要怎么操作,先“寻找一个新的免费域名”,点击“检查可用性”,会出来一些免费的域名.3.自己选其所好,找到一个加入购物车4.去购物车结帐,都是不要钱的.点击“继续”,5.这时候应该要求填写个人信息,这里提供随机生成身份.选择想要的信息.6.邮箱要填自己的,然后邮箱收件箱会收到一封邮件,要求你注册一下.7.你注册好后,在上面网站继续登录.你就会发现你有这个域名了（如果没有,你再搜一下,重新添加进你的购物车）8.有域名后,在你的博客source目录下,添加名字为CNAME（注意没有后缀）的文件,里面的内容就是你的域名.然后hexo g和hexo d下.继而进入DNSPOD把自己的域名解析一下.9.主要先登录,qq,微信都可以.登录上,进入管理控制台,添加自己的域名到域名解析.添加记录的每一项,系统都会提示代表意思,这里主要解释记录类型A记录:地址记录,用来指定域名的IP地址CNAME记录:如果需要将域名指向另一个域名,再由另一个域名提供IP地址,就需要添加CNAME记录NS记录:域名服务器记录,如果需要把子域名交给其他DNS服务商解析,就需要添加NS记录上面的NS记录是系统默认添加的.A记录就是指向对应IP地址,这里的192.30.252.153和192.30.252.154是github的服务器IP地址.CNAME记录这里可填可不填,因为A记录已经将mayi21.tk和mayi21.github.io的域名统一为一个IP地址了有一种情况就是为了提高访问速度,要区分国内国外不同用户使用不同的网站进行重定向需要添加对应的CNAME记录.10.然后继续输入你的name.github.io,就会跳转到你绑定的域名中. google收录1.打开谷歌搜索引擎验证2.输入自己的博客地址,添加属性,我用的是域名服务商验证,把google提供的文本,按照提示添加进入DNSPOD就行了3.点击验证,应该就可以成功了","categories":[{"name":"建站","slug":"建站","permalink":"https://mayi21.github.io/categories/建站/"}],"tags":[{"name":"hexo建站","slug":"hexo建站","permalink":"https://mayi21.github.io/tags/hexo建站/"}]}]}