{"meta":{"title":"wiki","subtitle":"my hearts will go on!","description":"坚持不懈的坚持","author":"Mayi21","url":"https://mayi21.github.io"},"pages":[{"title":"","date":"2018-09-21T13:21:42.644Z","updated":"2018-09-21T13:11:57.692Z","comments":true,"path":"googlea6b07bee95a649f7.html","permalink":"https://mayi21.github.io/googlea6b07bee95a649f7.html","excerpt":"","text":"google-site-verification: googlea6b07bee95a649f7.html"},{"title":"","date":"2018-09-21T13:20:28.484Z","updated":"2018-08-31T06:34:42.303Z","comments":true,"path":"404.html","permalink":"https://mayi21.github.io/404.html","excerpt":"","text":"404页面 *{margin:0;padding:0;outline:none;font-family:\\5FAE\\8F6F\\96C5\\9ED1,宋体;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;-khtml-user-select:none;user-select:none;cursor:default;font-weight:lighter;} .center{margin:0 auto;} .whole{width:100%;height:100%;line-height:100%;position:fixed;bottom:0;left:0;z-index:-1000;overflow:hidden;} .whole img{width:100%;height:100%;} .mask{width:100%;height:100%;position:absolute;top:0;left:0;background:#000;opacity:0.6;filter:alpha(opacity=60);} .b{width:100%;text-align:center;height:400px;position:absolute;top:50%;margin-top:-230px}.a{width:150px;height:50px;margin-top:30px}.a a{display:block;float:left;width:150px;height:50px;background:#fff;text-align:center;line-height:50px;font-size:18px;border-radius:25px;color:#333}.a a:hover{color:#000;box-shadow:#fff 0 0 20px} p{color:#fff;margin-top:40px;font-size:24px;} #num{margin:0 5px;font-weight:bold;} .plan{color: black;background: white;font-size: 30px; margin-top: 20px;} .plan:hover{color: white;background: black;font-size: 30px;} #gg { position: absolute; width: 654px; height: 470px; left: 50%; top: 50%; margin-left: -377px; margin-top: -235px; }"},{"title":"","date":"2018-08-31T05:51:21.039Z","updated":"2018-08-31T05:51:21.039Z","comments":true,"path":"categories/index.html","permalink":"https://mayi21.github.io/categories/index.html","excerpt":"","text":"title: 文章分类date: 2018-08-24 14:47:21type: “categories”"}],"posts":[{"title":"Spark-1","slug":"Spark","date":"2019-06-06T11:40:08.745Z","updated":"2019-06-09T13:09:24.955Z","comments":true,"path":"2019/06/06/Spark/","link":"","permalink":"https://mayi21.github.io/2019/06/06/Spark/","excerpt":"1.分布式数据的容错性处理较为常用的方法有检查节点和更新记录。检查节点的方法是对每个节点逐个进行检测，随时查询每个节点的运行情况。好处是便于主节点随时了解人物的真实数据运行情况，坏处就是资源消耗非常大，","text":"1.分布式数据的容错性处理较为常用的方法有检查节点和更新记录。检查节点的方法是对每个节点逐个进行检测，随时查询每个节点的运行情况。好处是便于主节点随时了解人物的真实数据运行情况，坏处就是资源消耗非常大，而且一旦出现问题，需要将数据在不同节点中搬运，反而更加耗费时间从而极大地拉低执行效率。更新记录指的是运行的主节点并不总是查询每个分节点的运行状态，而是将相同的数据存放在不同的节点上面（一般不超过三个），各个工作节点按固定的周期更新在主节点中的运行记录，如果在一定的时间内主节点查询到的数据的更新状态超时或者有异常，则在存储相同数据的不同节点上重新启动数据计算工作。缺点在于如果数据量过大，更新数据和重新启动运行任务的资源耗费也相当大。2.Spark架构Driver:运行Application的main函数并且创建SparkContext；Client:用户提交作业的客户端；Worker:集群中任何可以运行Application代码的节点，运行一个或多个Executor的进程；Executor:运行在worker的Task执行器，Executor启动线程池运行Task，并且负责将数据存在磁盘或者内存中。每个Application都会申请各自的Executor来处理任务；SparkContext:整个应用的上下文，控制应用的声明周期；RDD:Spark的基本计算单元，一组rdd形成执行的有向无环图RDD Graph；DAG Scheduler:根据job构建基于stage的dag工作流，并提交stage给taskscheduler；TaskScheduler:将Task分发给Executor执行；SparkEnv:线程级别的上下文，储存运行时的重要组件的引用。 华丽分割线 小测试中1.scala中的nothing是所有类的子类。2.ClusterManager是主节点。3.scala反编译后可读可执行，是字节码文件。","categories":[{"name":"Spark","slug":"Spark","permalink":"https://mayi21.github.io/categories/Spark/"}],"tags":[{"name":"Spark开发","slug":"Spark开发","permalink":"https://mayi21.github.io/tags/Spark开发/"}]},{"title":"Python-1","slug":"Python-1","date":"2019-06-06T10:28:46.040Z","updated":"2019-06-07T15:18:10.707Z","comments":true,"path":"2019/06/06/Python-1/","link":"","permalink":"https://mayi21.github.io/2019/06/06/Python-1/","excerpt":"Q.调用方法带括号和不带括号的区别？A.带括号返回的是计算结果，是一个值；不带括号返回的是这个函数体。","text":"Q.调用方法带括号和不带括号的区别？A.带括号返回的是计算结果，是一个值；不带括号返回的是这个函数体。 华丽分割线 1.list去重：List(Set(List)) 1.math库的高等特殊函数函数 | 数学表示 | 描述 | :-: | -math.erf(x) | $\\frac{2}{\\sqrt{\\pi}}\\lmoustache_{0}^{x}e^{-t^{2}}dt$ | 高斯误差，应用于概率论，统计学等领域math.erfc(x) | $\\frac{2}{\\sqrt{\\pi}}\\lmoustache_{0}^{\\infty}e^{-t^{2}}dt$ | 余补高斯误差函数，math.erfc(x)=1-math.erf(x)math.gamma(x) | $\\lmoustache_{0}^{\\infty}x^{t-1}e^{-x}dx$ | 伽玛函数，也叫欧拉第二积分函数math.lgamma(x) | ln(gamma(x)) | 伽马函数的自然对数","categories":[{"name":"Python","slug":"Python","permalink":"https://mayi21.github.io/categories/Python/"}],"tags":[{"name":"Python开发","slug":"Python开发","permalink":"https://mayi21.github.io/tags/Python开发/"}]},{"title":"java笔记-1","slug":"Java-1","date":"2019-05-22T10:47:49.286Z","updated":"2019-06-09T12:54:00.211Z","comments":true,"path":"2019/05/22/Java-1/","link":"","permalink":"https://mayi21.github.io/2019/05/22/Java-1/","excerpt":"Q.String,StringBuffer和StringBuilder的区别A.String是字符串常量，而StringBuilder和StringBuffer是字符串变量；由String创建的字符内容不可改变，而由StringBuffer和StringBuilder创建的字符内容可以改变；StringBuffer是线程安全的，","text":"Q.String,StringBuffer和StringBuilder的区别A.String是字符串常量，而StringBuilder和StringBuffer是字符串变量；由String创建的字符内容不可改变，而由StringBuffer和StringBuilder创建的字符内容可以改变；StringBuffer是线程安全的，StringBuilder是线程不安全的，但在可能的情况下先建议使用此，因为不需要考虑同步问题，所以会更快，String是线程安全的，但是线程安全主要是针对那些需要维护内部状态改变的类，而String是不可以改变的，所有操作都不会对它进行修改，所以一般不讨论String的线程安全问题。 Q.String类为什么要声明为final？A.这样就不能够继承，也就没法修改这个类的实现，就更加安全。 Q.netty和tomcat的区别在于？A.通信协议。Tomcat是基于http协议的，实质上是一个基于http的web容器；但是netty不一样，它能够通过编程自定义各种协议，因为netty能够通过自己的codec来解码/编码字节流。 Q.Thread使用run()和start()启动的区别？A.start():真正实现了多线程运行，通过调用Thread类的start()方法来启动一个线程，这时此线程是处于就绪状态，并没有运行。然后通过此Thread类调用方法run()来完成其运行操作的，这里方法run()称为线程体，它包含了要执行的这个线程的内容，Run方法运行结束，此线程终止。然后CPU再调度其它线程。run():不开启新的线程，只是按部就班的调用对象的run()方法，此时只有一个主线程，只有等run()方法执行完毕才会执行下面的代码。 Q.多次start一个线程会出现什么问题？A.正常启动一次后，再次启动抛出”java.lang.IllegalThreadStateException”的异常，在调用start方法时，会首先判断threadStatus是否等于0，如果不等于0，就抛出异常。 Q.线程安全的map？A.ConcurrentMap，SynchronizedMap，HashTable。HashTable的get和put方法都被synchronized关键字修饰，说明他们是方法级别的阻塞，它们占用共享资源锁，所以导致同时只能一个线程操作get或者put，而且get/put操作不能同时执行，所以这种同步的集合效率非常低，一般不建议使用这个集合。SynchronizedMap的实现方式是加了个对象锁，每次对HashMap的操作都要先获取这个mutex的对象锁才能进入，所以性能也不会比HashTable好到哪里去，也不建议使用。ConcurrentMap是最推荐使用的线程安全的Map，也是实现方式最复杂的一个集合，每个版本的实现方式也不一样，在jdk8之前是使用分段加锁的一个方式，分成16个桶，每次只加锁其中一个桶，而在jdk8又加入了红黑树和CAS算法来实现。 Q.foreach(增强型for循环的底层实现方式)？A.通过对代码123for (Integer i : list) &#123; System.out.println(i);&#125; 反编译得到1234Integer i;for(Iterator iterator = list.iterator(); iterator.hasNext(); System.out.println(i))&#123; i = (Integer)iterator.next(); &#125; 按照执行顺序进行拆解12345Integer i：定义一个临时整型变量i Iterator iterator = list.iterator()：获取迭代器iterator.hasNext()： 判断迭代器中是否有未遍历过的元素i = (Integer)iterator.next()： 获取第一个未遍历的元素，赋值给临时变量iSystem.out.println(i)：输出临时变量i的值 通过反编译得到，java底层的foreach循环是通过迭代器模式实现。(注意：使用迭代器删除元素时要注意使用不当可能发生ConcurrentModificationException异常,Iterator是工作在一个独立的线程中，并且拥有一个 mutex 锁。 Iterator被创建之后会建立一个指向原来对象的单链索引表，当原来的对象数量发生变化时，这个索引表的内容不会同步改变，所以当索引指针往后移动的时候就找不到要迭代的对象，所以按照 fail-fast 原则 Iterator 会马上抛出java.util.ConcurrentModificationException异常。所以 Iterator 在工作的时候是不允许被迭代的对象被改变的。但你可以使用 Iterator 本身的方法 remove() 来删除对象，Iterator.remove() 方法会在删除当前迭代对象的同时维护索引的一致性。正确的在遍历的同时删除元素的姿势：123456Iterator&lt;Student&gt; stuIter = students.iterator(); while (stuIter.hasNext()) &#123; Student student = stuIter.next(); if (student.getId() == 2) stuIter.remove();//这里要使用Iterator的remove方法移除当前对象，如果使用List的remove方法，则同样会出现ConcurrentModificationException &#125; )。 Q.MySql数据库索引的实现原理？A.MySql数据库支持多种索引类型，如BTree(平衡搜索多叉树)索引，B+Tree索引，哈希索引，全文索引 Q.静态代码块和主函数？A.静态代码块先执行，然后执行主函数；静态代码块中前面的先执行，后面的后执行。 Q.Session与Cookie的区别？A.Session是存放在服务器端，所以会占用服务器的资源，Cookie是存放在客户端；Cookie单个限制4k大小，Session没有大小限制。 Q.什么是序列化？什么情况下需要序列化？为什么HttpSession中对象要序列化？那些不会序列化？A.为了保存内存中对象的状态，并且可以把保存的对象状态再读出来，虽然自己有各种各样的办法保存，但是java提供一种官方的保存方法，那就是序列化；当你想把内存中的对象保存到文件或者数据库时，当你想用RMI(Remote Method Invocation)传输对象时，当你想用套接字在网络上传输对象时；因为Session是用来传输各种值和对象的，对象是不能通过网络传输，所以必须序列化。(相关注意事项：当一个父类实现序列化，子类自动实现序列化，不需要显示实现Serializable接口；当一个对象的实例变量引用其他对象，序列化该对象时，也把引用的对象序列化；并非所有的对象都可以序列化。)；当使用transient关键字时，是不会序列化，还有static关键字不会序列化，因为声明为static的变量是属于类的，而序列化是保存对象的状态。 Q.RMI(Remote Method Invocation)与RPC(Remote Procedure Call Protocol)的区别？A.适用语言范围不同：RPC设计在应用程序间通信的平台中立的方式，不会理会操作系统之间的差异，即支持多种语言。RMI是基于java虚拟机的，所以只支持java语言。调用结果的返回形式不同：RPC不支持对象的概念，传送到RPC服务的消息由外部数据表示语言表示，这种语言抽象了字节序类和数据类型之间的差异RMI允许返回Java对象以及基本数据类型方法调用方式不同：RMI中是通过在客户端的Stub对象作为远程接口进行远程方法的调用。每个远程方法都具有方法签名。如果一个方法在服务器上执行，但是没有相匹配的签名被添加到这个远程接口(stub)上，那么这个新方法就不能被RMI客户方所调用。RPC中是通过网络服务协议向远程主机发送请求，请求包含了一个参数集和一个文本值，通常形成“classname.methodname(参数集)”的形式。RPC远程主机就去搜索与之相匹配的类和方法，找到后就执行方法并把结果编码，通过网络协议发回。 Q.HashMap怎么put()和get()元素？A.HashMap的put实现:根据key的hashcode计算key值的hash值，根据hash值插入到表中的位置，如果该位置有元素，便以链表的方式存储，新加入的元素放在链表头；最先加入的元素放在链表尾。HashMap的get实现:根据key的hashcode值计算hash值，然后找到在表中的位置，然后通过equals方法，逐个寻找在链表中的位置。 Q.Collections.sort()和Arrays.sort()采用哪种排序方式？A.Collections.sort()采用增强型归并排序法；Arrays.sort()针对基本数据类型采用快速排序法，对象数组使用归并排序。 Q.HashSet怎样保证所存元素单一？A.根据hash码和equals方法：如果hash码值不相同，则说明元素不存在，就存入；如果hash码值相同则进行equals方法判断是否为同一对象，如果判断相等，就不存，如果判断不等，就存入。(hash值是根据hashcode计算到的)。 Q.设计模式A.设计模式是一套被反复使用，多数人知晓的，经过分类的，代码设计经验总结。使用设计模式的目的：为了代码的重用性，让代码更容易被别人理解，保证代码可靠性。设计模式分为三大类：创建型模式，结构型模式，行为型模式。创建型模式：工厂方法模式，抽象工厂模式，单例模式，建造者模式，原型模式。结构型模式：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。行为型模式：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。其实还有两类：并发行模式和线程池模式。 华丽的分割线 transient关键字：Java的serialization提供了一种持久化对象实例的机制。当持久化对象时，可能有一个特殊的对象数据成员，我们不想用serialization机制来保存它。为了在一个特定对象的一个域上关闭serialization，可以在这个域前加上关键字transient。当一个对象被序列化的时候，transient型变量的值不包括在序列化的表示中，然而非transient型的变量是被包括进去的。 volatile关键字：volatile仅仅用来保证该变量对所有线程的可见性，但不保证原子性。可见性：声明为volatile变量的值在修改时，所有线程都知道这个变量的值修改了。不保证原子性：修改变量的值实际上分为好几步，而在这几步内是不安全的volatile作为java中的关键词之一，用以声明变量的值可能随时会别的线程修改，使用volatile修饰的变量会强制将修改的值立即写入主存，主存中值的更新会使缓存中的值失效(非volatile变量不具备这样的特性，非volatile变量的值会被缓存，线程A更新了这个值，线程B读取这个变量的值时可能读到的并不是是线程A更新后的值)。volatile会禁止指令重排。 华丽分割线 聊一聊HashMap：HashMap实现了Map，Cloneable，Serializable三个接口，继承了AbstractMap， 聊一聊List，Set，Map存放null值问题。List 集合可以存储null，添加几个，存储几个；Set集合也可以存储null，但只能存储一个，即使添加多个也只能存储一个；HashMap可以存储null键值对，键和值都可以是null，但如果添加的键值对的键相同，则后面添加的键值对会覆盖前面的键值对，即之后存储后添加的键值对；Hashtable不能碰null，不管是值还是键，一见null就报空指针。 IO：传统的IO分为四类：InputStream，OutputStream的基于字节的IOReader和Writer的基于字符的IOFile基于磁盘的IOSocket基于网络的IOBIO,NIO,AIO的区别。BIO 就是传统的 java.io 包，它是基于流模型实现的，交互的方式是同步、阻塞方式，也就是说在读入输入流或者输出流时，在读写动作完成之前，线程会一直阻塞在那里，它们之间的调用时可靠的线性顺序。它的有点就是代码比较简单、直观；缺点就是 IO 的效率和扩展性很低，容易成为应用性能瓶颈。NIO 是 Java 1.4 引入的 java.nio 包，提供了 Channel、Selector、Buffer 等新的抽象，可以构建多路复用的、同步非阻塞 IO 程序，同时提供了更接近操作系统底层高性能的数据操作方式。AIO 是 Java 1.7 之后引入的包，是 NIO 的升级版本，提供了异步非堵塞的 IO 操作方式，所以人们叫它 AIO（Asynchronous IO），异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。同步和异步同步就是一个任务的完成需要依赖另外一个任务时，只有等待被依赖的任务完成后，依赖的任务才能算完成，这是一种可靠的任务序列。要么成功都成功，失败都失败，两个任务的状态可以保持一致。而异步是不需要等待被依赖的任务完成，只是通知被依赖的任务要完成什么工作，依赖的任务也立即执行，只要自己完成了整个任务就算完成了。至于被依赖的任务最终是否真正完成，依赖它的任务无法确定，所以它是不可靠的任务序列。我们可以用打电话和发短信来很好的比喻同步与异步操作。阻塞和非阻塞阻塞与非阻塞主要是从 CPU 的消耗上来说的，阻塞就是 CPU 停下来等待一个慢的操作完成 CPU 才接着完成其它的事。非阻塞就是在这个慢的操作在执行时 CPU 去干其它别的事，等这个慢的操作完成时，CPU 再接着完成后续的操作。虽然表面上看非阻塞的方式可以明显的提高 CPU 的利用率，但是也带了另外一种后果就是系统的线程切换增加。增加的 CPU 使用时间能不能补偿系统的切换成本需要好好评估。 同步/异步，阻塞/非阻塞组合组合方式 | 性能分析-| -同步阻塞 | 最常用的用法，使用简单，IO性能一般很差，CPU大部分处于空闲状态同步非阻塞 | 在网络IO是长连接，同时传输数据不是很多的情况下，提升性能非常有效，但会增加CPU消耗，要考虑增加IO性能能不能补偿CPU消耗，也就是系统的瓶颈是在IO还是CPU上异步阻塞 | 在分布式数据经常用，对于同时写多份相同数据的情况下，异步阻塞对于网络IO能够提升效率异步非阻塞 | 组合方式比较复杂，一般在一些非常复杂的分布式情况下使用，像集群之间的消息同步机制，它适合同时要传多份相同的数据到集群的不同机器，同时数据传输量虽然不大，但是却非常频繁，用这种方式性能能够达到顶峰 传统IO面向流，使用简单，但是性能不足，IO的各种流是阻塞的，当一个线程调用read() or write()时，线程是阻塞的。BIO：同步：发起一个调用后，被调用者未处理完请求后，调用不返回。异步：发起一个调用后，立即得到被调用者的回应表示已接收到请求，但是被调用者并没有返回结果，此时我们可以处理其他请求，被调用者通常依靠事件，回调等机制通知调用者返回结果同步和异步的区别最大在于异步的话调用者不需要等待处理结果，被调用者会通过回调等机制来通知调用者其返回结果。阻塞：阻塞就是发起一个请求，调用者一直等待请求结果返回，也就是当前线程会被挂起，无法从事其他任务，只有当条件就绪才能继续。非阻塞：非阻塞就是发起一个请求，调用者不用一直等着结果返回，可以先去干其他事情。socketServer的accept方法是阻塞的。当有连接请求时，socketServer通过accept方法获取一个socket，取得socket后，将这个socket分给一个线程去处理。此时socket需要等待有效的请求数据到来后，才可以真正开始处理请求。socket交给线程后，这时socketServer才可以接收下一个连接请求。获得连接的顺序是和客户端请求到达服务器的先后顺序相关。NIO:基于事件驱动，当有连接请求，会将此连接注册到多路复用器上（selector）。在多路复用器上可以注册监听事件，比如监听accept、read。通过监听，当真正有请求数据时，才来处理数据。不会阻塞，会不停的轮询是否有就绪的事件，所以处理顺序和连接请求先后顺序无关，与请求数据到来的先后顺序有关BIO与NIO对比：主要对比BIO一个连接，一个线程，非http请求，有可能只连接不发请求数据，此时线程是无用浪费的。BIO处理依赖于连接建立；NIO处理依赖于请求数据的到来。导致执行顺序不同。一个线程处理一个请求BIO：连接请求来，建立socket，等待请求数据到来（t1），处理时间（t2）NIO：连接请求来，注册到selector，设置读监听，等待请求数据（t1），处理时间（t2）此时，两者用时皆为t1+t2，没有区别一个线程处理两个请求第一个请求，等待请求数据（10），处理时间（1）第二个请求，等待请求数据（1），处理时间（2）BIO：用时 10+1+1+2=14，第1个执行完用时10+1，等待第一个执行完处理第2个，用时1+2NIO：用时 1+2+7+1=11， 第二个数据先到，时间 1+2，此时第一个需要等时为10秒，还没到，还需等待7秒，时间为7+1两个线程处理两个请求第一个请求，等待请求数据（10），处理时间（1）第二个请求，等待请求数据（1），处理时间（2）BIO：用时 10+1+2=13，等待第1个请求10，交给工作线程一处理，此时同时接受第2个，等待1秒，处理时间2秒，此间线程一处理时间为一秒，在线程二结束之前就已经结束NIO：用时 1+2+7+1=11，第二个数据先到，时间 1+2，此时第一个还没到，还需等待7秒，时间为7+1如果两个请求顺序相反，则bio和nio一样，都是11秒由此可见由于阻塞等待机制的不同，导致效率不同，主要优化点为，不必排队等待，先到先处理，就有可能效率高一点。BIO如果想要处理并发请求，则必须使用多线程，一般后端会用线程池来支持NIO可以使用单线程，可以减少线程切换上下文的消耗。但是虽然单线程减少了线程切换的消耗，但是处理也变为线性的，也就是处理完一个请求，才能处理第二个。这时，有这么两个场景：后端是密集型的计算，没有大量的IO操作，比如读些文件、数据库等后端是有大量的IO操作。当为第一种场景时：NIO单线程则比较有优势， 理由是虽然是单线程，但是由于线程的计算是并发计算，不是并行计算，说到底，计算压力还是在CPU上，一个线程计算，没有线程的多余消耗，显然比NIO多线程要高效。BIO则必为多线程，否则将阻塞到天荒地老，但多线程是并发，不是并行，主要还是依靠CPU的线性计算，另外还有处理大量的线程上下文。如果为第二种场景，多线程将有一定优势，多个线程把等待IO的时间能平均开。此时两者区别主要取决于以上分析的处理顺序了，显然NIO要更胜一筹。总结NIO在接收请求方式上，无疑是要高效于BIO，原因并非是不阻塞，我认为NIO一样是阻塞的，只是方式不同，先来的有效请求先处理，先阻塞时间短的。此时间可用于等待等待时间长的。在处理请求上，NIO和BIO并没有什么不同，主要看线程池规划是否和理。NIO相对BIO在密集型计算的模型下，可以用更少的线程，甚至单线程。https://blog.csdn.net/wy0123/article/details/79382761 分布式算法：轮询算法，哈希算法，最少连接算法，相应速度算法，加权法等，常用的就是哈希算法。哈希算法就是对hash结果取余数(hash() mod N):对机器编号从0到N-1，按照自定义的 hash()算法，对每个请求的hash()值按N取模，得到余数i，然后将请求分发到编号为i的机器。但这样的算法方法存在致命问题，如果某一台机器宕机，那么应该落在该机器的请求就无法得到正确的处理，这时需要将当掉的服务器从算法从去除，此时候会有(N-1)/N的服务器的缓存数据需要重新进行计算;如果新增一台机器，会有N /(N+1)的服务器的缓存数据需要进行重新计算。对于系统而言，这通常是不可接受的颠簸(因为这意味着大量缓存的失效或者数据需要转移)。那么，如何设计一个负载均衡策略，使得受到影响的请求尽可能的少呢?一致性哈希将整个哈希值空间按照顺时针方向组织成一个虚拟的环，下一步将服务器的ip或者关键字进行哈希，映射到环上，然后根据数据的key计算哈希同时映射到环上，从此位置顺时针走，遇到的第一个服务器就是该数据所存放的位置。删除或者增加服务器受到影响的只是一部分数据。当服务器过少，且两台服务器在环上的位置比较近，则容易造成数据倾斜的问题，在前的一个服务器得到大量的数据，为了解决这种数据倾斜的问题，我们将一个服务器节点计算多个哈希，每个计算结果的位置都放在环上，称为虚拟节点，这样就能够抑制数据倾斜问题。 ConCurrentHashMap解析：JDK1.8的底层是散列表+红黑树，支持高并发访问和更新，他是线程安全的，key和value都不允许为空。JDK1.7的实现是segments+HashEntry数组。有了Hashtable为啥需要ConCurrentHashMap？Hashtable是在每个方法上都加上了Synchronized完成同步，效率低下。ConcurrentHashMap通过在部分加锁和利用CAS算法来实现同步。 Netty小知识。netty具有并发高，传输快，封装好。并发高是因为netty基于NIO","categories":[{"name":"Java","slug":"Java","permalink":"https://mayi21.github.io/categories/Java/"}],"tags":[{"name":"Java开发","slug":"Java开发","permalink":"https://mayi21.github.io/tags/Java开发/"}]},{"title":"Centos配置hadoop和spark","slug":"Linux配置hadoop与spark","date":"2019-03-23T13:05:17.152Z","updated":"2019-03-24T03:59:35.673Z","comments":true,"path":"2019/03/23/Linux配置hadoop与spark/","link":"","permalink":"https://mayi21.github.io/2019/03/23/Linux配置hadoop与spark/","excerpt":"前言:大数据学生一枚,hadoop是基础,spark是进阶,到了学spark的时候了,又得重新配置一台服务器了.正好借这次机会,把配置步骤记下来,省得以后再翻老班发的PPT.环境:Centos7 64位,JDK1.8 hadoop2.8.4,scala2.11.6,spark2.4.0","text":"前言:大数据学生一枚,hadoop是基础,spark是进阶,到了学spark的时候了,又得重新配置一台服务器了.正好借这次机会,把配置步骤记下来,省得以后再翻老班发的PPT.环境:Centos7 64位,JDK1.8 hadoop2.8.4,scala2.11.6,spark2.4.01.先安装通用工具:yum install lrzsz.安装完成后在Xshell中使用rz命令可选择上传文件,十分方便.2.配置网关:vim /etc/sysconfig/network,应该是空的,添加内容如下HOSTNAME=zzti GATEWAY=192.168.17.2,网关设置也可能不是,自己可能要改一下.3.修改主机名称:vim /etc/hosts,在最前面添加127.0.0.1 zzti. 然后关闭防火墙systemctl stop firewalld 查看防火墙状态的命令:systemctl status firewalld.4.此时重启一下,倒杯水,准备下面一番苦干吧.reboot.5.创建工作目录:mkdir -p /data/zzti ln -s /data/zzti/ /zzti useradd zzti chown zzti:zzti /zzti.6.配置免密登录:ssh-keygen,一路回车到底,然后执行cat ~/.ssh/id_rsa.pub &gt; ~/.ssh/authorized_keys chmod 644 ~/.ssh/authorized_keys mkdir /home/zzti/.ssh cp ~/.ssh/* /home/zzti/.ssh/ chown -R zzti:zzti /home/zzti/.ssh chmod 700 /home/zzti/.ssh,此过程应该没什么错误.下面操作在zzti用户下完成 su zzti7.上传JDK（此处省略无数字,自己找jdk,上传命令rz）,然后解压tar -zxvf jdk-8u181-linux-x64.tar.gz,更换自己的jdk版本,建立软连接,方便操作ln -s jdk1.8.0_181/ jdk.8.上传hadoop,然后解压tar -zxvf hadoop-2.8.4.tar.gz,建立软连接 ln -s hadoop-2.8.4 hadoop.配置环境变量:vim ~/.bashrc,在最后面加上如下的内容:123export JAVA_HOME=/zzti/jdk/ export HADOOP_HOME=/zzti/hadoop/export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin 然后退出保存,执行source ~/.bashrc,使其生效,然后运行java -version,看到jdk的版本即为成功.9.配置hadoop:cd /zzti/hadoop/etc/hadoop,在core-site.xml的两个configure标签添加如下内容:12345678910111213141516&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/zzti/data/hadoop_tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://zzti:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;zzti:2181&lt;/value&gt; &lt;/property&gt; 保存退出后,继续编辑hdfs-site.xml,vim hdfs-site.xml,在configure标签之间添加如下内容:123456789101112131415161718192021222324&lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;zzti-cluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;zzti:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///zzti/data/hadoop/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///zzti/data/hadoop/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; 保存退出.配置mapred-site.xml,因为只有mapred-site.xml.template,先cp一下cp mapred-site.xml.template mapred-site.xml,然后再编辑vim mapred-site.xml,然后老规矩,添加如下内容:12345678910111213141516&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobtracker.http.address&lt;/name&gt; &lt;value&gt;zzti:50030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;zzti:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;zzti:19888&lt;/value&gt; &lt;/property&gt; 保存退出.编辑yarn-site.xml,vim yarn-site.xml,老规矩添加入下内容:1234&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 到此,配置结束.10.配置hdfs:创建文件夹mkdir /zzti/data/hadoop/name /zzti/data/hadoop/data,echo zzti &gt; slaves cd /zzti/hadoop/bin ./hadoop namenode –format cd /zzti/hadoop/sbin ./start-all.sh,完成后,输入jps,查看是否出现NameNode,NodeManager,DataNode,ResourceManager,Jps,出现这些即为配置成功.cd /zzti/hadoop/bin 创建hdfs文件夹hadoop fs -mkdir /data,查看是否创建成功hadoop fs -ls /,出现data目录即为创建成功.11.功能测试:就自行进行,在此不做演示.hadoop安装完成下面进行spark的安装依然在zzti用户下操作1.创建目录:mkdir libs,进入libs,cd libs,上传spark和scala,解压sparktar -zxvf spark-2.4.0-bin-without-hadoop.tgz,建立软连接ln -s spark-2.4.0-bin-without-hadoop spark,解压scalatar -zxvf scala-2.11.6.tgz,创建软连接ln -s scala-2.11.6 scala.2.配置环境变量vim ~/.bashrc,添加如下内容:12export SPARK_HOME=/zzti/libs/sparkexport SCALA_HOME=/zzti/libs/scala 在PATH后面添加:$SCALA_HOME/bin:$SPARK_HOME/bin,保存退出.source ~/.bashrc使其生效.3.配置spark,在spark的conf的目录下,mv slaves.template slaves,echo zzti &gt; slaves mv spark-env.sh.template spark-env.sh,编辑spark-env.sh vim spark-env.sh,在文档最后加入如下内容:1234567891011121314export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport YARN_CONF_DIR=$HADOOP_HOME/etc/hadoopexport SPARK_LIBARY_PATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$HADOOP_HOME/lib/nativeexport SPARK_CLASSPATH=$(/zzti/hadoop/bin/hadoop classpath)export SPARK_DIST_CLASSPATH=$(/zzti/hadoop/bin/hadoop classpath)export SPARK_EXECUTOR_INSTANCES=1export SPARK_EXECUTOR_CORES=1export SPARK_DRIVER_MEMORY=512Mexport SPARK_EXECUTOR_MEMORY=512Mexport SPARK_MASTER_HOST=zztiexport SPARK_MASTER_WEBUI_PORT=28686export SPARK_LOCAL_DIRS=/data/zzti/data/spark/localexport SPARK_WORKER_DIR=/data/zzti/data/spark/workexport SPARK_LOG_DIR=/data/zzti/logs/spark 保存退出.4.启动sparkbash /zzti/libs/spark/sbin/start-all.sh,然后输入spark-shell,即可进入操作.","categories":[{"name":"Linux","slug":"Linux","permalink":"https://mayi21.github.io/categories/Linux/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://mayi21.github.io/tags/hadoop/"}]},{"title":"创建支持ssh的镜像服务","slug":"创建支持ssh服务的镜像","date":"2019-03-11T11:18:14.028Z","updated":"2019-03-23T14:24:03.576Z","comments":true,"path":"2019/03/11/创建支持ssh服务的镜像/","link":"","permalink":"https://mayi21.github.io/2019/03/11/创建支持ssh服务的镜像/","excerpt":"1.先安装docker,这里不再赘述2.使用ubuntu镜像来创建容器:docker pull ubuntu,然后启动容器:docker run -it ubuntu /bin/bash","text":"1.先安装docker,这里不再赘述2.使用ubuntu镜像来创建容器:docker pull ubuntu,然后启动容器:docker run -it ubuntu /bin/bash3.更新一下软件源:apt-get update apt-get upgrade,然后进行安装apt-get install openssh-server4.要是想正常启动服务,需要目录/var/run/sshd存在,手动创建,并启动服务:mkdir -p /var/run/sshd /usr/sbin/sshd -D &amp;5.查看容器的22端口:netstat -tunlp,如果提示命令没有安装,则选择安装:apt-get install net-tools,然后在执行命令就行了.此时端口应该是处于监听状态.6.在root用户目录下创建.ssh目录,并复制需要登录的公钥信息（一般在本地主机用户目录下的.ssh/id_rsa.pub）,先创建文件夹:mkdir root/.ssh,然后复制公钥信息到authorized_keys中,vim /root/.ssh/authorized_keys7.创建自启动ssh服务的可执行文件run.sh,并添加可执行权限:vim /run.sh chmod +x run.sh,run.sh脚本的内容为:#!/bin/bash/usr/sbin/sshd -D8.使用Ctrl + p + q退出但并不关闭容器.9.用docker commit命令保存为一个新的sshd:ubuntu镜像:docker commit &lt;Image Id&gt; sshd:ubuntu,使用docker images查看本地的镜像10.启动容器,并添加映射端口10022 -&gt; 22,其中10022是宿主机的端口,22是容器的SSH服务监听端口:docker run -p 10022:22 -d sshd:ubuntu /run.sh,启动成功后,在宿主机上看到容器的运行的详细信息.11.在宿主机（192.168.1.200）上或其他主机,通过ssh服务访问10022端口来登录容器:ssh 192.168.1.200 -p 10022参考:Docker技术入门与实践","categories":[{"name":"docker","slug":"docker","permalink":"https://mayi21.github.io/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://mayi21.github.io/tags/docker/"}]},{"title":"利用Tensorflow构建自定义的图片分类器","slug":"Tensorflow构建自定义的图片分类器","date":"2019-02-17T10:24:22.525Z","updated":"2019-03-23T14:23:24.441Z","comments":true,"path":"2019/02/17/Tensorflow构建自定义的图片分类器/","link":"","permalink":"https://mayi21.github.io/2019/02/17/Tensorflow构建自定义的图片分类器/","excerpt":"操作环境:Ubuntu 18.04 Tensorflow 1.71.安装配置Tensorflow或者利用Docker.Tensorflow的安装这里不予多介绍,主要介绍一下Docker的用法.Docker的操作命令如下:","text":"操作环境:Ubuntu 18.04 Tensorflow 1.71.安装配置Tensorflow或者利用Docker.Tensorflow的安装这里不予多介绍,主要介绍一下Docker的用法.Docker的操作命令如下: docker pull tensorflow/tensorflow#指定宿主机的目录和容器的目录,实现文件共享docker run -it -v /root:/root tensorflow/tensorflow2.克隆Gitgit clone https://github.com/googlecodelabs/tensorflow-for-poets-2cd tensorflow-for-poets-23.准备训练数据,将这些数据存放在tf_files文件夹下,这些数据要求:这个tf_files/&lt;自定义文件夹&gt;下面是子文件夹,子文件夹下面是同种类型的图片(如果是人,那该文件下就是这个人的所有图片,子文件夹的名字就是这些图片的名字)4.准备训练数据.定义图片的大小,export IMAGE_SIZE=224export ARCHITECTURE=&quot;mobilenet_0.50_${IMAGE_SIZE}&quot;开始训练.python -m scripts.retrain --bottleneck_dir=tf_files/bottlenecks --how_many_training_steps=500 --model_dir=tf_files/models/ --summaries_dir=tf_files/training_summaries/&quot;${ARCHITECTURE}&quot; --output_graph=tf_files/retrained_graph.pb --output_labels=tf_files/retrained_labels.txt --architecture=&quot;${ARCHITECTURE}&quot; --image_dir=tf_files/&lt;自定义的文件夹&gt;–how_many_training_steps=500 这个是训练多少步.这里可以不用这个命令,使用默认步数4000步5.验证数据.python -m scripts.label_image --graph=tf_files/retrained_graph.pb --image=&lt;自己想检测的图片&gt;然后下面就会出现验证的结果.","categories":[{"name":"python","slug":"python","permalink":"https://mayi21.github.io/categories/python/"}],"tags":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"https://mayi21.github.io/tags/Tensorflow/"}]},{"title":"idea开发hadoop","slug":"idea开发hadoop","date":"2018-12-03T08:54:53.797Z","updated":"2019-03-24T04:02:39.560Z","comments":true,"path":"2018/12/03/idea开发hadoop/","link":"","permalink":"https://mayi21.github.io/2018/12/03/idea开发hadoop/","excerpt":"下载idea和hadopp源码包新建一个java project,用于hadoop开发,同时需要导入hadoop的相关依赖包java project建成后,点击project structure,进入Dependencies,点击右边的+,点击JARs or","text":"下载idea和hadopp源码包新建一个java project,用于hadoop开发,同时需要导入hadoop的相关依赖包java project建成后,点击project structure,进入Dependencies,点击右边的+,点击JARs ordirectories…,添加依赖,这些依赖都可以在share/hadoop下面找到12345commonhdfsmapreduceyarn common/lib 然后是Artifacts,点击+,添加JAR&gt;Empty,名称自定义,然后点击+,点击Module output,在弹出的对话框选择当前的项目,点击保存.接着新建一个Application,Edit Configurations,点击+,新建一个Application,在Main Class中填入org.apache.hadoop.util.RunJarWorking directory当然是选择当前项目的目录了, Program arguments 这个是设置默认参数的会在程序执行的时候传递进去1234/home/edmond/workspace/IdeaProjects/Hadoop/HadoopWordCount/out/artifacts/wordcount/wordcount.jarcom.company.Maininputoutput 第一个是jar包所在的位置第二个是Main函数所在的类第三四两个参数是由自己决定的（这两个参数会作为args[0]和args[1]传入）点击ok保存.自己写mapper和reducer测试吧！","categories":[{"name":"Java","slug":"Java","permalink":"https://mayi21.github.io/categories/Java/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://mayi21.github.io/tags/hadoop/"}]},{"title":"配置远程访问jupyter","slug":"配置远程访问jupyter","date":"2018-07-21T09:58:08.241Z","updated":"2019-06-08T09:04:09.361Z","comments":true,"path":"2018/07/21/配置远程访问jupyter/","link":"","permalink":"https://mayi21.github.io/2018/07/21/配置远程访问jupyter/","excerpt":"##前言:jupyternotebook是一款在线的python编程环境.交互式的编程环境,支持四十多种语言.对于爬虫和分析数据,这样的操作十分方便.众所周知:Jupyter是一个WEB应用.本篇文章教大家在服务器端部署Jupyter并远程访问.1.服务器环境:大家可以自行在网上购买服务器,本篇文章使用的服务器环境是Ubuntu 16.04.1 LTS x86_64.2.安全规则:添加安全规则,开放8888端口,记得同时开放22端口,以便在putty,xshell等软件上登陆;然后关联服务器实例.","text":"##前言:jupyternotebook是一款在线的python编程环境.交互式的编程环境,支持四十多种语言.对于爬虫和分析数据,这样的操作十分方便.众所周知:Jupyter是一个WEB应用.本篇文章教大家在服务器端部署Jupyter并远程访问.1.服务器环境:大家可以自行在网上购买服务器,本篇文章使用的服务器环境是Ubuntu 16.04.1 LTS x86_64.2.安全规则:添加安全规则,开放8888端口,记得同时开放22端口,以便在putty,xshell等软件上登陆;然后关联服务器实例.3.使用wget下载anaconda,在清华大学开源软件镜像站寻找相应的版本,复制下载链接,然后wget便可.4.使用sh Anaconda3-5.1.0-Linux-x86_64.sh安装Anaconda,安装一路yes便可,安装完成后重启终端,尝试conda.5.输入jupyter notebook --generate-config,生成默认的jupyter配置文件6.安装ipython,sudo apt-get install ipython3.7.启动ipythonipthon,输入from notebook.auth import passwd回车,输入passwd(),此时要求输入密码,这个密码为登陆服务器端jupyter的密码.两次输入后得到密钥(以sha开头的).8.vim /home/ubuntu/.jupyter/jupyter_notebook_config.py9.在# Configuration file for jupyter-notebook.后另起一行添加1234c.NotebookApp.ip=&apos;*&apos;c.NotebookApp.password = u&apos;密钥&apos;c.NotebookApp.open_browser = Falsec.NotebookApp.port =8888 保存退出.10.安装自动补码,在jupyter notebook未运行的情况下操作12python -m pip install jupyter_contrib_nbextensionsjupyter contrib nbextension install --user --skip-running-check 然后在 Clusters旁边多出 Nbextensions点击进去,找到 Hinterland,打勾即可10.运行Jupyter Notebook jupyter notebook,后台运行nohup jupyter notebook --allow-root &gt; jnotebook.out 2&gt;&amp;1 &amp;11.在本地电脑浏览器上输入[http://you server ip:8888]12.输入上面设置的密码.","categories":[{"name":"python","slug":"python","permalink":"https://mayi21.github.io/categories/python/"}],"tags":[{"name":"Python开发","slug":"Python开发","permalink":"https://mayi21.github.io/tags/Python开发/"}]},{"title":"hexo建站","slug":"hexo建站第一步-1","date":"2018-06-26T14:16:33.965Z","updated":"2019-03-23T14:21:46.239Z","comments":true,"path":"2018/06/26/hexo建站第一步-1/","link":"","permalink":"https://mayi21.github.io/2018/06/26/hexo建站第一步-1/","excerpt":"1.下载git和node.js2.打开GitBash3.下载并安装hexo","text":"1.下载git和node.js2.打开GitBash3.下载并安装hexo4.新建一个文件夹,hexo inital先初始化一下该目录5.自己找合适的主题6.在git bash中运行hexo g生成本地文件,hexo d将其发布到github上面7.在source/_posts/下面写文章,我用的是MarkdownPad2,只要熟悉一下语法就行 今天主要谈一下绑定域名1.申请域名,闲着没事干,免费申请了一年的域名,freenom2.介绍一下主要怎么操作,先“寻找一个新的免费域名”,点击“检查可用性”,会出来一些免费的域名.3.自己选其所好,找到一个加入购物车4.去购物车结帐,都是不要钱的.点击“继续”,5.这时候应该要求填写个人信息,这里提供随机生成身份.选择想要的信息.6.邮箱要填自己的,然后邮箱收件箱会收到一封邮件,要求你注册一下.7.你注册好后,在上面网站继续登录.你就会发现你有这个域名了（如果没有,你再搜一下,重新添加进你的购物车）8.有域名后,在你的博客source目录下,添加名字为CNAME（注意没有后缀）的文件,里面的内容就是你的域名.然后hexo g和hexo d下.继而进入DNSPOD把自己的域名解析一下.9.主要先登录,qq,微信都可以.登录上,进入管理控制台,添加自己的域名到域名解析.添加记录的每一项,系统都会提示代表意思,这里主要解释记录类型A记录:地址记录,用来指定域名的IP地址CNAME记录:如果需要将域名指向另一个域名,再由另一个域名提供IP地址,就需要添加CNAME记录NS记录:域名服务器记录,如果需要把子域名交给其他DNS服务商解析,就需要添加NS记录上面的NS记录是系统默认添加的.A记录就是指向对应IP地址,这里的192.30.252.153和192.30.252.154是github的服务器IP地址.CNAME记录这里可填可不填,因为A记录已经将mayi21.tk和mayi21.github.io的域名统一为一个IP地址了有一种情况就是为了提高访问速度,要区分国内国外不同用户使用不同的网站进行重定向需要添加对应的CNAME记录.10.然后继续输入你的name.github.io,就会跳转到你绑定的域名中. google收录1.打开谷歌搜索引擎验证2.输入自己的博客地址,添加属性,我用的是域名服务商验证,把google提供的文本,按照提示添加进入DNSPOD就行了3.点击验证,应该就可以成功了","categories":[{"name":"建站","slug":"建站","permalink":"https://mayi21.github.io/categories/建站/"}],"tags":[{"name":"hexo建站","slug":"hexo建站","permalink":"https://mayi21.github.io/tags/hexo建站/"}]}]}