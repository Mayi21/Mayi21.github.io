{"meta":{"title":"wiki","subtitle":"my hearts will go on!","description":"坚持不懈的坚持","author":"Mayi21","url":"https://mayi21.github.io"},"pages":[{"title":"","date":"2018-09-21T13:21:42.644Z","updated":"2018-09-21T13:11:57.692Z","comments":true,"path":"googlea6b07bee95a649f7.html","permalink":"https://mayi21.github.io/googlea6b07bee95a649f7.html","excerpt":"","text":"google-site-verification: googlea6b07bee95a649f7.html"},{"title":"","date":"2018-09-21T13:20:28.484Z","updated":"2018-08-31T06:34:42.303Z","comments":true,"path":"404.html","permalink":"https://mayi21.github.io/404.html","excerpt":"","text":"404页面 *{margin:0;padding:0;outline:none;font-family:\\5FAE\\8F6F\\96C5\\9ED1,宋体;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;-khtml-user-select:none;user-select:none;cursor:default;font-weight:lighter;} .center{margin:0 auto;} .whole{width:100%;height:100%;line-height:100%;position:fixed;bottom:0;left:0;z-index:-1000;overflow:hidden;} .whole img{width:100%;height:100%;} .mask{width:100%;height:100%;position:absolute;top:0;left:0;background:#000;opacity:0.6;filter:alpha(opacity=60);} .b{width:100%;text-align:center;height:400px;position:absolute;top:50%;margin-top:-230px}.a{width:150px;height:50px;margin-top:30px}.a a{display:block;float:left;width:150px;height:50px;background:#fff;text-align:center;line-height:50px;font-size:18px;border-radius:25px;color:#333}.a a:hover{color:#000;box-shadow:#fff 0 0 20px} p{color:#fff;margin-top:40px;font-size:24px;} #num{margin:0 5px;font-weight:bold;} .plan{color: black;background: white;font-size: 30px; margin-top: 20px;} .plan:hover{color: white;background: black;font-size: 30px;} #gg { position: absolute; width: 654px; height: 470px; left: 50%; top: 50%; margin-left: -377px; margin-top: -235px; }"},{"title":"","date":"2018-08-31T05:51:21.039Z","updated":"2018-08-31T05:51:21.039Z","comments":true,"path":"categories/index.html","permalink":"https://mayi21.github.io/categories/index.html","excerpt":"","text":"title: 文章分类date: 2018-08-24 14:47:21type: “categories”"}],"posts":[{"title":"Spark-1","slug":"Spark","date":"2019-06-06T11:40:08.745Z","updated":"2019-06-07T15:18:20.340Z","comments":true,"path":"2019/06/06/Spark/","link":"","permalink":"https://mayi21.github.io/2019/06/06/Spark/","excerpt":"1.分布式数据的容错性处理较为常用的方法有检查节点和更新记录。检查节点的方法是对每个节点逐个进行检测，随时查询每个节点的运行情况。好处是便于主节点随时了解人物的真实数据运行情况，坏处就是资源消耗非常大，","text":"1.分布式数据的容错性处理较为常用的方法有检查节点和更新记录。检查节点的方法是对每个节点逐个进行检测，随时查询每个节点的运行情况。好处是便于主节点随时了解人物的真实数据运行情况，坏处就是资源消耗非常大，而且一旦出现问题，需要将数据在不同节点中搬运，反而更加耗费时间从而极大地拉低执行效率。更新记录指的是运行的主节点并不总是查询每个分节点的运行状态，而是将相同的数据存放在不同的节点上面（一般不超过三个），各个工作节点按固定的周期更新在主节点中的运行记录，如果在一定的时间内主节点查询到的数据的更新状态超时或者有异常，则在存储相同数据的不同节点上重新启动数据计算工作。缺点在于如果数据量过大，更新数据和重新启动运行任务的资源耗费也相当大。2.Spark架构Driver:运行Application的main函数并且创建SparkContext；Client:用户提交作业的客户端；Worker:集群中任何可以运行Application代码的节点，运行一个或多个Executor的进程；Executor:运行在worker的Task执行器，Executor启动线程池运行Task，并且负责将数据存在磁盘或者内存中。每个Application都会申请各自的Executor来处理任务；SparkContext:整个应用的上下文，控制应用的声明周期；RDD:Spark的基本计算单元，一组rdd形成执行的有向无环图RDD Graph；DAG Scheduler:根据job构建基于stage的dag工作流，并提交stage给taskscheduler；TaskScheduler:将Task分发给Executor执行；SparkEnv:线程级别的上下文，储存运行时的重要组件的引用。 华丽分割线 小测试中1.scala中的nothing是所有类的子类。2.ClusterManager是主节点。3.scala反编译后可读可执行，是字节码文件。","categories":[{"name":"Spark","slug":"Spark","permalink":"https://mayi21.github.io/categories/Spark/"}],"tags":[{"name":"Spark开发","slug":"Spark开发","permalink":"https://mayi21.github.io/tags/Spark开发/"}]},{"title":"Python-1","slug":"Python-1","date":"2019-06-06T10:28:46.040Z","updated":"2019-06-07T15:18:10.707Z","comments":true,"path":"2019/06/06/Python-1/","link":"","permalink":"https://mayi21.github.io/2019/06/06/Python-1/","excerpt":"Q.调用方法带括号和不带括号的区别？A.带括号返回的是计算结果，是一个值；不带括号返回的是这个函数体。","text":"Q.调用方法带括号和不带括号的区别？A.带括号返回的是计算结果，是一个值；不带括号返回的是这个函数体。 华丽分割线 1.list去重：List(Set(List)) 1.math库的高等特殊函数函数 | 数学表示 | 描述 | :-: | -math.erf(x) | $\\frac{2}{\\sqrt{\\pi}}\\lmoustache_{0}^{x}e^{-t^{2}}dt$ | 高斯误差，应用于概率论，统计学等领域math.erfc(x) | $\\frac{2}{\\sqrt{\\pi}}\\lmoustache_{0}^{\\infty}e^{-t^{2}}dt$ | 余补高斯误差函数，math.erfc(x)=1-math.erf(x)math.gamma(x) | $\\lmoustache_{0}^{\\infty}x^{t-1}e^{-x}dx$ | 伽玛函数，也叫欧拉第二积分函数math.lgamma(x) | ln(gamma(x)) | 伽马函数的自然对数","categories":[{"name":"Python","slug":"Python","permalink":"https://mayi21.github.io/categories/Python/"}],"tags":[{"name":"Python开发","slug":"Python开发","permalink":"https://mayi21.github.io/tags/Python开发/"}]},{"title":"java笔记-1","slug":"Java-1","date":"2019-05-22T10:47:49.286Z","updated":"2019-06-08T08:29:09.815Z","comments":true,"path":"2019/05/22/Java-1/","link":"","permalink":"https://mayi21.github.io/2019/05/22/Java-1/","excerpt":"Q.String,StringBuffer和StringBuilder的区别A.String是字符串常量，而StringBuilder和StringBuffer是字符串变量；由String创建的字符内容不可改变，而由StringBuffer和StringBuilder创建的字符内容可以改变；StringBuffer是线程安全的，","text":"Q.String,StringBuffer和StringBuilder的区别A.String是字符串常量，而StringBuilder和StringBuffer是字符串变量；由String创建的字符内容不可改变，而由StringBuffer和StringBuilder创建的字符内容可以改变；StringBuffer是线程安全的，StringBuilder是线程不安全的，但在可能的情况下先建议使用此，因为不需要考虑同步问题，所以会更快，String是线程安全的，但是线程安全主要是针对那些需要维护内部状态改变的类，而String是不可以改变的，所有操作都不会对它进行修改，所以一般不讨论String的线程安全问题。Q.多次start一个线程会出现什么问题？A.正常启动一次后，再次启动抛出”java.lang.IllegalThreadStateException”的异常，在调用start方法时，会首先判断threadStatus是否等于0，如果不等于0，就抛出异常。Q.线程安全的map？A.ConcurrentMap，SynchronizedMap，HashTable。HashTable的get和put方法都被synchronized关键字修饰，说明他们是方法级别的阻塞，它们占用共享资源锁，所以导致同时只能一个线程操作get或者put，而且get/put操作不能同时执行，所以这种同步的集合效率非常低，一般不建议使用这个集合。看出SynchronizedMap的实现方式是加了个对象锁，每次对HashMap的操作都要先获取这个mutex的对象锁才能进入，所以性能也不会比HashTable好到哪里去，也不建议使用。这个也是最推荐使用的线程安全的Map，也是实现方式最复杂的一个集合，每个版本的实现方式也不一样，在jdk8之前是使用分段加锁的一个方式，分成16个桶，每次只加锁其中一个桶，而在jdk8又加入了红黑树和CAS算法来实现。Q.foreach(增强型for循环的底层实现方式)？A.通过对代码123for (Integer i : list) &#123; System.out.println(i);&#125; 反编译得到1234Integer i;for(Iterator iterator = list.iterator(); iterator.hasNext(); System.out.println(i))&#123; i = (Integer)iterator.next(); &#125; 按照执行顺序进行拆解12345Integer i：定义一个临时整型变量i Iterator iterator = list.iterator()：获取迭代器iterator.hasNext()： 判断迭代器中是否有未遍历过的元素i = (Integer)iterator.next()： 获取第一个未遍历的元素，赋值给临时变量iSystem.out.println(i)：输出临时变量i的值 通过反编译得到，java底层的foreach循环是通过迭代器模式实现。(注意：使用迭代器删除元素时要注意使用不当可能发生ConcurrentModificationException异常,Iterator是工作在一个独立的线程中，并且拥有一个 mutex 锁。 Iterator被创建之后会建立一个指向原来对象的单链索引表，当原来的对象数量发生变化时，这个索引表的内容不会同步改变，所以当索引指针往后移动的时候就找不到要迭代的对象，所以按照 fail-fast 原则 Iterator 会马上抛出java.util.ConcurrentModificationException异常。所以 Iterator 在工作的时候是不允许被迭代的对象被改变的。但你可以使用 Iterator 本身的方法 remove() 来删除对象，Iterator.remove() 方法会在删除当前迭代对象的同时维护索引的一致性。正确的在遍历的同时删除元素的姿势：123456Iterator&lt;Student&gt; stuIter = students.iterator(); while (stuIter.hasNext()) &#123; Student student = stuIter.next(); if (student.getId() == 2) stuIter.remove();//这里要使用Iterator的remove方法移除当前对象，如果使用List的remove方法，则同样会出现ConcurrentModificationException &#125; )。Q.MySql数据库索引的实现原理？A.MySql数据库支持多种索引类型，如BTree(平衡搜索多叉树)索引，B+Tree索引，哈希索引，全文索引Q.静态代码块和主函数？A.静态代码块先执行，然后执行主函数；静态代码块中前面的先执行，后面的后执行。Q.为什么要用到序列化？什么情况下需要序列化？为什么HttpSession中对象要序列化？A.为了保存内存中对象的状态，并且可以把保存的对象状态再读出来，虽然自己有各种各样的办法保存，但是java提供一种官方的保存方法，那就是序列化；当你想把内存中的对象保存到文件或者数据库时，当你想用RMI(Remote Method Invocation)传输对象时，当你想用套接字在网络上传输对象时；因为Session是用来传输各种值和对象的，对象是不能通过网络传输，所以必须序列化。(相关注意事项：当一个父类实现序列化，子类自动实现序列化，不需要显示实现Serializable接口；当一个对象的实例变量引用其他对象，序列化该对象时，也把引用的对象序列化；并非所有的对象都可以序列化。)Q.RMI(Remote Method Invocation)与RPC(Remote Procedure Call Protocol)的区别？A.适用语言范围不同：RPC设计在应用程序间通信的平台中立的方式，不会理会操作系统之间的差异，即支持多种语言。RMI是基于java虚拟机的，所以只支持java语言。调用结果的返回形式不同：RPC不支持对象的概念，传送到RPC服务的消息由外部数据表示语言表示，这种语言抽象了字节序类和数据类型之间的差异RMI允许返回Java对象以及基本数据类型方法调用方式不同：RMI中是通过在客户端的Stub对象作为远程接口进行远程方法的调用。每个远程方法都具有方法签名。如果一个方法在服务器上执行，但是没有相匹配的签名被添加到这个远程接口(stub)上，那么这个新方法就不能被RMI客户方所调用。RPC中是通过网络服务协议向远程主机发送请求，请求包含了一个参数集和一个文本值，通常形成“classname.methodname(参数集)”的形式。RPC远程主机就去搜索与之相匹配的类和方法，找到后就执行方法并把结果编码，通过网络协议发回。Q.HashMap怎么put()和get()元素？A.HashMap的put实现:根据key的hashcode计算key值的hash值，根据hash值插入到表中的位置，如果该位置有元素，便以链表的方式存储，新加入的元素放在链表头；最先加入的元素放在链表尾。HashMap的get实现:根据key的hashcode值计算hash值，然后找到在表中的位置，然后通过equals方法，逐个寻找在链表中的位置。Q.Collections.sort()和Arrays.sort()采用哪种排序方式？A.Collections.sort()采用增强型归并排序法；Arrays.sort()针对基本数据类型采用快速排序法，对象数组使用归并排序。Q.HashSet怎样保证所存元素单一？A.根据hash码和equals方法：如果hash码值不相同，则说明元素不存在，就存入；如果hash码值相同则进行equals方法判断是否为同一对象，如果判断相等，就不存，如果判断不等，就存入。(hash值是根据hashcode计算到的)。Q.设计模式A.设计模式是一套被反复使用，多数人知晓的，经过分类的，代码设计经验总结。使用设计模式的目的：为了代码的重用性，让代码更容易被别人理解，保证代码可靠性。设计模式分为三大类：创建型模式，结构型模式，行为型模式。创建型模式：工厂方法模式，抽象工厂模式，单例模式，建造者模式，原型模式。结构型模式：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。行为型模式：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。其实还有两类：并发行模式和线程池模式。 华丽的分割线 transient关键字：Java的serialization提供了一种持久化对象实例的机制。当持久化对象时，可能有一个特殊的对象数据成员，我们不想用serialization机制来保存它。为了在一个特定对象的一个域上关闭serialization，可以在这个域前加上关键字transient。当一个对象被序列化的时候，transient型变量的值不包括在序列化的表示中，然而非transient型的变量是被包括进去的。volatile关键字：volatile作为java中的关键词之一，用以声明变量的值可能随时会别的线程修改，使用volatile修饰的变量会强制将修改的值立即写入主存，主存中值的更新会使缓存中的值失效(非volatile变量不具备这样的特性，非volatile变量的值会被缓存，线程A更新了这个值，线程B读取这个变量的值时可能读到的并不是是线程A更新后的值)。volatile会禁止指令重排。 华丽分割线 BTree","categories":[{"name":"Java","slug":"Java","permalink":"https://mayi21.github.io/categories/Java/"}],"tags":[{"name":"Java开发","slug":"Java开发","permalink":"https://mayi21.github.io/tags/Java开发/"}]},{"title":"Centos配置hadoop和spark","slug":"Linux配置hadoop与spark","date":"2019-03-23T13:05:17.152Z","updated":"2019-03-24T03:59:35.673Z","comments":true,"path":"2019/03/23/Linux配置hadoop与spark/","link":"","permalink":"https://mayi21.github.io/2019/03/23/Linux配置hadoop与spark/","excerpt":"前言:大数据学生一枚,hadoop是基础,spark是进阶,到了学spark的时候了,又得重新配置一台服务器了.正好借这次机会,把配置步骤记下来,省得以后再翻老班发的PPT.环境:Centos7 64位,JDK1.8 hadoop2.8.4,scala2.11.6,spark2.4.0","text":"前言:大数据学生一枚,hadoop是基础,spark是进阶,到了学spark的时候了,又得重新配置一台服务器了.正好借这次机会,把配置步骤记下来,省得以后再翻老班发的PPT.环境:Centos7 64位,JDK1.8 hadoop2.8.4,scala2.11.6,spark2.4.01.先安装通用工具:yum install lrzsz.安装完成后在Xshell中使用rz命令可选择上传文件,十分方便.2.配置网关:vim /etc/sysconfig/network,应该是空的,添加内容如下HOSTNAME=zzti GATEWAY=192.168.17.2,网关设置也可能不是,自己可能要改一下.3.修改主机名称:vim /etc/hosts,在最前面添加127.0.0.1 zzti. 然后关闭防火墙systemctl stop firewalld 查看防火墙状态的命令:systemctl status firewalld.4.此时重启一下,倒杯水,准备下面一番苦干吧.reboot.5.创建工作目录:mkdir -p /data/zzti ln -s /data/zzti/ /zzti useradd zzti chown zzti:zzti /zzti.6.配置免密登录:ssh-keygen,一路回车到底,然后执行cat ~/.ssh/id_rsa.pub &gt; ~/.ssh/authorized_keys chmod 644 ~/.ssh/authorized_keys mkdir /home/zzti/.ssh cp ~/.ssh/* /home/zzti/.ssh/ chown -R zzti:zzti /home/zzti/.ssh chmod 700 /home/zzti/.ssh,此过程应该没什么错误.下面操作在zzti用户下完成 su zzti7.上传JDK（此处省略无数字,自己找jdk,上传命令rz）,然后解压tar -zxvf jdk-8u181-linux-x64.tar.gz,更换自己的jdk版本,建立软连接,方便操作ln -s jdk1.8.0_181/ jdk.8.上传hadoop,然后解压tar -zxvf hadoop-2.8.4.tar.gz,建立软连接 ln -s hadoop-2.8.4 hadoop.配置环境变量:vim ~/.bashrc,在最后面加上如下的内容:123export JAVA_HOME=/zzti/jdk/ export HADOOP_HOME=/zzti/hadoop/export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin 然后退出保存,执行source ~/.bashrc,使其生效,然后运行java -version,看到jdk的版本即为成功.9.配置hadoop:cd /zzti/hadoop/etc/hadoop,在core-site.xml的两个configure标签添加如下内容:12345678910111213141516&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/zzti/data/hadoop_tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://zzti:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;zzti:2181&lt;/value&gt; &lt;/property&gt; 保存退出后,继续编辑hdfs-site.xml,vim hdfs-site.xml,在configure标签之间添加如下内容:123456789101112131415161718192021222324&lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;zzti-cluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;zzti:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///zzti/data/hadoop/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///zzti/data/hadoop/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; 保存退出.配置mapred-site.xml,因为只有mapred-site.xml.template,先cp一下cp mapred-site.xml.template mapred-site.xml,然后再编辑vim mapred-site.xml,然后老规矩,添加如下内容:12345678910111213141516&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobtracker.http.address&lt;/name&gt; &lt;value&gt;zzti:50030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;zzti:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;zzti:19888&lt;/value&gt; &lt;/property&gt; 保存退出.编辑yarn-site.xml,vim yarn-site.xml,老规矩添加入下内容:1234&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 到此,配置结束.10.配置hdfs:创建文件夹mkdir /zzti/data/hadoop/name /zzti/data/hadoop/data,echo zzti &gt; slaves cd /zzti/hadoop/bin ./hadoop namenode –format cd /zzti/hadoop/sbin ./start-all.sh,完成后,输入jps,查看是否出现NameNode,NodeManager,DataNode,ResourceManager,Jps,出现这些即为配置成功.cd /zzti/hadoop/bin 创建hdfs文件夹hadoop fs -mkdir /data,查看是否创建成功hadoop fs -ls /,出现data目录即为创建成功.11.功能测试:就自行进行,在此不做演示.hadoop安装完成下面进行spark的安装依然在zzti用户下操作1.创建目录:mkdir libs,进入libs,cd libs,上传spark和scala,解压sparktar -zxvf spark-2.4.0-bin-without-hadoop.tgz,建立软连接ln -s spark-2.4.0-bin-without-hadoop spark,解压scalatar -zxvf scala-2.11.6.tgz,创建软连接ln -s scala-2.11.6 scala.2.配置环境变量vim ~/.bashrc,添加如下内容:12export SPARK_HOME=/zzti/libs/sparkexport SCALA_HOME=/zzti/libs/scala 在PATH后面添加:$SCALA_HOME/bin:$SPARK_HOME/bin,保存退出.source ~/.bashrc使其生效.3.配置spark,在spark的conf的目录下,mv slaves.template slaves,echo zzti &gt; slaves mv spark-env.sh.template spark-env.sh,编辑spark-env.sh vim spark-env.sh,在文档最后加入如下内容:1234567891011121314export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport YARN_CONF_DIR=$HADOOP_HOME/etc/hadoopexport SPARK_LIBARY_PATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$HADOOP_HOME/lib/nativeexport SPARK_CLASSPATH=$(/zzti/hadoop/bin/hadoop classpath)export SPARK_DIST_CLASSPATH=$(/zzti/hadoop/bin/hadoop classpath)export SPARK_EXECUTOR_INSTANCES=1export SPARK_EXECUTOR_CORES=1export SPARK_DRIVER_MEMORY=512Mexport SPARK_EXECUTOR_MEMORY=512Mexport SPARK_MASTER_HOST=zztiexport SPARK_MASTER_WEBUI_PORT=28686export SPARK_LOCAL_DIRS=/data/zzti/data/spark/localexport SPARK_WORKER_DIR=/data/zzti/data/spark/workexport SPARK_LOG_DIR=/data/zzti/logs/spark 保存退出.4.启动sparkbash /zzti/libs/spark/sbin/start-all.sh,然后输入spark-shell,即可进入操作.","categories":[{"name":"Linux","slug":"Linux","permalink":"https://mayi21.github.io/categories/Linux/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://mayi21.github.io/tags/hadoop/"}]},{"title":"创建支持ssh的镜像服务","slug":"创建支持ssh服务的镜像","date":"2019-03-11T11:18:14.028Z","updated":"2019-03-23T14:24:03.576Z","comments":true,"path":"2019/03/11/创建支持ssh服务的镜像/","link":"","permalink":"https://mayi21.github.io/2019/03/11/创建支持ssh服务的镜像/","excerpt":"1.先安装docker,这里不再赘述2.使用ubuntu镜像来创建容器:docker pull ubuntu,然后启动容器:docker run -it ubuntu /bin/bash","text":"1.先安装docker,这里不再赘述2.使用ubuntu镜像来创建容器:docker pull ubuntu,然后启动容器:docker run -it ubuntu /bin/bash3.更新一下软件源:apt-get update apt-get upgrade,然后进行安装apt-get install openssh-server4.要是想正常启动服务,需要目录/var/run/sshd存在,手动创建,并启动服务:mkdir -p /var/run/sshd /usr/sbin/sshd -D &amp;5.查看容器的22端口:netstat -tunlp,如果提示命令没有安装,则选择安装:apt-get install net-tools,然后在执行命令就行了.此时端口应该是处于监听状态.6.在root用户目录下创建.ssh目录,并复制需要登录的公钥信息（一般在本地主机用户目录下的.ssh/id_rsa.pub）,先创建文件夹:mkdir root/.ssh,然后复制公钥信息到authorized_keys中,vim /root/.ssh/authorized_keys7.创建自启动ssh服务的可执行文件run.sh,并添加可执行权限:vim /run.sh chmod +x run.sh,run.sh脚本的内容为:#!/bin/bash/usr/sbin/sshd -D8.使用Ctrl + p + q退出但并不关闭容器.9.用docker commit命令保存为一个新的sshd:ubuntu镜像:docker commit &lt;Image Id&gt; sshd:ubuntu,使用docker images查看本地的镜像10.启动容器,并添加映射端口10022 -&gt; 22,其中10022是宿主机的端口,22是容器的SSH服务监听端口:docker run -p 10022:22 -d sshd:ubuntu /run.sh,启动成功后,在宿主机上看到容器的运行的详细信息.11.在宿主机（192.168.1.200）上或其他主机,通过ssh服务访问10022端口来登录容器:ssh 192.168.1.200 -p 10022参考:Docker技术入门与实践","categories":[{"name":"docker","slug":"docker","permalink":"https://mayi21.github.io/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://mayi21.github.io/tags/docker/"}]},{"title":"利用Tensorflow构建自定义的图片分类器","slug":"Tensorflow构建自定义的图片分类器","date":"2019-02-17T10:24:22.525Z","updated":"2019-03-23T14:23:24.441Z","comments":true,"path":"2019/02/17/Tensorflow构建自定义的图片分类器/","link":"","permalink":"https://mayi21.github.io/2019/02/17/Tensorflow构建自定义的图片分类器/","excerpt":"操作环境:Ubuntu 18.04 Tensorflow 1.71.安装配置Tensorflow或者利用Docker.Tensorflow的安装这里不予多介绍,主要介绍一下Docker的用法.Docker的操作命令如下:","text":"操作环境:Ubuntu 18.04 Tensorflow 1.71.安装配置Tensorflow或者利用Docker.Tensorflow的安装这里不予多介绍,主要介绍一下Docker的用法.Docker的操作命令如下: docker pull tensorflow/tensorflow#指定宿主机的目录和容器的目录,实现文件共享docker run -it -v /root:/root tensorflow/tensorflow2.克隆Gitgit clone https://github.com/googlecodelabs/tensorflow-for-poets-2cd tensorflow-for-poets-23.准备训练数据,将这些数据存放在tf_files文件夹下,这些数据要求:这个tf_files/&lt;自定义文件夹&gt;下面是子文件夹,子文件夹下面是同种类型的图片(如果是人,那该文件下就是这个人的所有图片,子文件夹的名字就是这些图片的名字)4.准备训练数据.定义图片的大小,export IMAGE_SIZE=224export ARCHITECTURE=&quot;mobilenet_0.50_${IMAGE_SIZE}&quot;开始训练.python -m scripts.retrain --bottleneck_dir=tf_files/bottlenecks --how_many_training_steps=500 --model_dir=tf_files/models/ --summaries_dir=tf_files/training_summaries/&quot;${ARCHITECTURE}&quot; --output_graph=tf_files/retrained_graph.pb --output_labels=tf_files/retrained_labels.txt --architecture=&quot;${ARCHITECTURE}&quot; --image_dir=tf_files/&lt;自定义的文件夹&gt;–how_many_training_steps=500 这个是训练多少步.这里可以不用这个命令,使用默认步数4000步5.验证数据.python -m scripts.label_image --graph=tf_files/retrained_graph.pb --image=&lt;自己想检测的图片&gt;然后下面就会出现验证的结果.","categories":[{"name":"python","slug":"python","permalink":"https://mayi21.github.io/categories/python/"}],"tags":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"https://mayi21.github.io/tags/Tensorflow/"}]},{"title":"idea开发hadoop","slug":"idea开发hadoop","date":"2018-12-03T08:54:53.797Z","updated":"2019-03-24T04:02:39.560Z","comments":true,"path":"2018/12/03/idea开发hadoop/","link":"","permalink":"https://mayi21.github.io/2018/12/03/idea开发hadoop/","excerpt":"下载idea和hadopp源码包新建一个java project,用于hadoop开发,同时需要导入hadoop的相关依赖包java project建成后,点击project structure,进入Dependencies,点击右边的+,点击JARs or","text":"下载idea和hadopp源码包新建一个java project,用于hadoop开发,同时需要导入hadoop的相关依赖包java project建成后,点击project structure,进入Dependencies,点击右边的+,点击JARs ordirectories…,添加依赖,这些依赖都可以在share/hadoop下面找到12345commonhdfsmapreduceyarn common/lib 然后是Artifacts,点击+,添加JAR&gt;Empty,名称自定义,然后点击+,点击Module output,在弹出的对话框选择当前的项目,点击保存.接着新建一个Application,Edit Configurations,点击+,新建一个Application,在Main Class中填入org.apache.hadoop.util.RunJarWorking directory当然是选择当前项目的目录了, Program arguments 这个是设置默认参数的会在程序执行的时候传递进去1234/home/edmond/workspace/IdeaProjects/Hadoop/HadoopWordCount/out/artifacts/wordcount/wordcount.jarcom.company.Maininputoutput 第一个是jar包所在的位置第二个是Main函数所在的类第三四两个参数是由自己决定的（这两个参数会作为args[0]和args[1]传入）点击ok保存.自己写mapper和reducer测试吧！","categories":[{"name":"Java","slug":"Java","permalink":"https://mayi21.github.io/categories/Java/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://mayi21.github.io/tags/hadoop/"}]},{"title":"配置远程访问jupyter","slug":"配置远程访问jupyter","date":"2018-07-21T09:58:08.241Z","updated":"2019-03-24T04:04:07.145Z","comments":true,"path":"2018/07/21/配置远程访问jupyter/","link":"","permalink":"https://mayi21.github.io/2018/07/21/配置远程访问jupyter/","excerpt":"##前言:jupyternotebook是一款在线的python编程环境.交互式的编程环境,支持四十多种语言.对于爬虫和分析数据,这样的操作十分方便.众所周知:Jupyter是一个WEB应用.本篇文章教大家在服务器端部署Jupyter并远程访问.1.服务器环境:大家可以自行在网上购买服务器,本篇文章使用的服务器环境是Ubuntu 16.04.1 LTS x86_64.2.安全规则:添加安全规则,开放8888端口,记得同时开放22端口,以便在putty,xshell等软件上登陆;然后关联服务器实例.","text":"##前言:jupyternotebook是一款在线的python编程环境.交互式的编程环境,支持四十多种语言.对于爬虫和分析数据,这样的操作十分方便.众所周知:Jupyter是一个WEB应用.本篇文章教大家在服务器端部署Jupyter并远程访问.1.服务器环境:大家可以自行在网上购买服务器,本篇文章使用的服务器环境是Ubuntu 16.04.1 LTS x86_64.2.安全规则:添加安全规则,开放8888端口,记得同时开放22端口,以便在putty,xshell等软件上登陆;然后关联服务器实例.3.使用wget下载anaconda,在清华大学开源软件镜像站寻找相应的版本,复制下载链接,然后wget便可.4.使用sh Anaconda3-5.1.0-Linux-x86_64.sh安装Anaconda,安装一路yes便可,安装完成后重启终端,尝试conda.5.输入jupyter notebook --generate-config,生成默认的jupyter配置文件6.安装ipython,sudo apt-get install ipython3.7.启动ipythonipthon,输入from notebook.auth import passwd回车,输入passwd(),此时要求输入密码,这个密码为登陆服务器端jupyter的密码.两次输入后得到密钥(以sha开头的).8.vim /home/ubuntu/.jupyter/jupyter_notebook_config.py9.在# Configuration file for jupyter-notebook.后另起一行添加1234c.NotebookApp.ip=&apos;*&apos;c.NotebookApp.password = u&apos;密钥&apos;c.NotebookApp.open_browser = Falsec.NotebookApp.port =8888 保存退出.10.安装自动补码,在jupyter notebook未运行的情况下操作12python -m pip install jupyter_contrib_nbextensionsjupyter contrib nbextension install --user --skip-running-check 然后在 Clusters旁边多出 Nbextensions点击进去,找到 Hinterland,打勾即可10.运行Jupyter Notebook jupyter notebook,后台运行nohup jupyter notebook --allow-root &gt; jnotebook.out 2&gt;&amp;1 &amp;11.在本地电脑浏览器上输入[http://you server ip:8888]12.输入上面设置的密码.","categories":[{"name":"python","slug":"python","permalink":"https://mayi21.github.io/categories/python/"}],"tags":[{"name":"python开发","slug":"python开发","permalink":"https://mayi21.github.io/tags/python开发/"}]},{"title":"hexo建站","slug":"hexo建站第一步-1","date":"2018-06-26T14:16:33.965Z","updated":"2019-03-23T14:21:46.239Z","comments":true,"path":"2018/06/26/hexo建站第一步-1/","link":"","permalink":"https://mayi21.github.io/2018/06/26/hexo建站第一步-1/","excerpt":"1.下载git和node.js2.打开GitBash3.下载并安装hexo","text":"1.下载git和node.js2.打开GitBash3.下载并安装hexo4.新建一个文件夹,hexo inital先初始化一下该目录5.自己找合适的主题6.在git bash中运行hexo g生成本地文件,hexo d将其发布到github上面7.在source/_posts/下面写文章,我用的是MarkdownPad2,只要熟悉一下语法就行 今天主要谈一下绑定域名1.申请域名,闲着没事干,免费申请了一年的域名,freenom2.介绍一下主要怎么操作,先“寻找一个新的免费域名”,点击“检查可用性”,会出来一些免费的域名.3.自己选其所好,找到一个加入购物车4.去购物车结帐,都是不要钱的.点击“继续”,5.这时候应该要求填写个人信息,这里提供随机生成身份.选择想要的信息.6.邮箱要填自己的,然后邮箱收件箱会收到一封邮件,要求你注册一下.7.你注册好后,在上面网站继续登录.你就会发现你有这个域名了（如果没有,你再搜一下,重新添加进你的购物车）8.有域名后,在你的博客source目录下,添加名字为CNAME（注意没有后缀）的文件,里面的内容就是你的域名.然后hexo g和hexo d下.继而进入DNSPOD把自己的域名解析一下.9.主要先登录,qq,微信都可以.登录上,进入管理控制台,添加自己的域名到域名解析.添加记录的每一项,系统都会提示代表意思,这里主要解释记录类型A记录:地址记录,用来指定域名的IP地址CNAME记录:如果需要将域名指向另一个域名,再由另一个域名提供IP地址,就需要添加CNAME记录NS记录:域名服务器记录,如果需要把子域名交给其他DNS服务商解析,就需要添加NS记录上面的NS记录是系统默认添加的.A记录就是指向对应IP地址,这里的192.30.252.153和192.30.252.154是github的服务器IP地址.CNAME记录这里可填可不填,因为A记录已经将mayi21.tk和mayi21.github.io的域名统一为一个IP地址了有一种情况就是为了提高访问速度,要区分国内国外不同用户使用不同的网站进行重定向需要添加对应的CNAME记录.10.然后继续输入你的name.github.io,就会跳转到你绑定的域名中. google收录1.打开谷歌搜索引擎验证2.输入自己的博客地址,添加属性,我用的是域名服务商验证,把google提供的文本,按照提示添加进入DNSPOD就行了3.点击验证,应该就可以成功了","categories":[{"name":"建站","slug":"建站","permalink":"https://mayi21.github.io/categories/建站/"}],"tags":[{"name":"hexo建站","slug":"hexo建站","permalink":"https://mayi21.github.io/tags/hexo建站/"}]}]}